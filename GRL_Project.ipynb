{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-whRxX9fqFB"
      },
      "source": [
        "# Alleviating Over-Smoothing in Graph Convolutional Networks Through Residual Weighted Shortest Path Neighborhood Aggregation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWKFAg_fAcF3"
      },
      "source": [
        "## Set up dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwY9iUPs9zAj",
        "outputId": "871e82a4-c99f-4cab-9776-0f312ab856b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import torch; print(torch.__version__)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iHJmiBqbAmTD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!sudo apt-get install texlive-latex-extra texlive-fonts-recommended dvipng cm-super\n",
        "!pip install torchviz\n",
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvsTj8ibBP3f"
      },
      "source": [
        "## Imports and Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xof0zSsNBSvd",
        "outputId": "be19375e-c27f-43ea-bd93-d1bb21a80341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:87: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:98: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:125: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import os\n",
        "import typing\n",
        "import torch_geometric\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.datasets as datasets\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch import Tensor\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from matplotlib import gridspec\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the Cora dataset\n",
        "dataset = datasets.Planetoid(\n",
        "    root=\"./\",\n",
        "    name='Cora',\n",
        "    split=\"public\",\n",
        "    transform=None\n",
        "  )\n",
        "\n",
        "print(dataset.data)\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load dataset\n",
        "data = dataset.data\n",
        "data = data.to(device)\n",
        "\n",
        "val_mask = dataset.data.val_mask.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54TDoY2HflAD"
      },
      "source": [
        "## Model definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwXtZN9BCjB7"
      },
      "source": [
        "### Define Vanilla GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VsBNV0ZgCl9R"
      },
      "outputs": [],
      "source": [
        "class GCN(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_dim: int,\n",
        "      hid_dim: int,\n",
        "      n_classes: int,\n",
        "      n_layers: int,\n",
        "      dropout_ratio: float = 0.3):\n",
        "    super(GCN, self).__init__()\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      input_dim: input feature dimension\n",
        "      hid_dim: hidden feature dimension\n",
        "      n_classes: number of target classes\n",
        "      n_layers: number of layers\n",
        "      dropout_ratio: dropout_ratio\n",
        "    \"\"\"\n",
        "    self.n_layers = n_layers\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "\n",
        "    # Define layers\n",
        "    if n_layers == 0:\n",
        "        self.fc = nn.Linear(input_dim, n_classes)\n",
        "    else:\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(GCNConv(input_dim, hid_dim))\n",
        "        for _ in range(n_layers - 1):\n",
        "            self.convs.append(GCNConv(hid_dim, hid_dim))\n",
        "        self.out_layer = nn.Linear(hid_dim, n_classes)\n",
        "\n",
        "  def forward(self, X, A) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the classification task.\n",
        "        \"\"\"\n",
        "        if self.n_layers == 0:\n",
        "            return self.fc(X)\n",
        "        else:\n",
        "            h = self.generate_node_embeddings(X, A)\n",
        "            logits = self.out_layer(h)  # Final layer for classification\n",
        "            return logits\n",
        "\n",
        "  def generate_node_embeddings(self, X, A) -> Tensor:\n",
        "      \"\"\"\n",
        "      Generate node embeddings for all layers.\n",
        "      \"\"\"\n",
        "      if self.n_layers == 0:\n",
        "          return self.fc(X)\n",
        "      else:\n",
        "          h = X\n",
        "          for conv in self.convs:\n",
        "              h = conv(h, A)  # Apply GCNConv\n",
        "              h = F.relu(h)  # Apply ReLU\n",
        "              h = F.dropout(h, p=self.dropout_ratio, training=self.training)  # Apply dropout\n",
        "          return h\n",
        "\n",
        "  def param_init(self):\n",
        "      \"\"\"\n",
        "      Initialize parameters for the model.\n",
        "      \"\"\"\n",
        "      if self.n_layers == 0:\n",
        "        self.fc.reset_parameters()\n",
        "      else:\n",
        "        for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "        self.out_layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs3Rdco5TyO1"
      },
      "source": [
        "### Define GCN with Skip-Connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1G5Iziuffa8r"
      },
      "outputs": [],
      "source": [
        "class SkipGCN(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_dim: int,\n",
        "      hid_dim: int,\n",
        "      n_classes: int,\n",
        "      n_layers: int,\n",
        "      dropout_ratio: float = 0.3):\n",
        "    super(SkipGCN, self).__init__()\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      input_dim: input feature dimension\n",
        "      hid_dim: hidden feature dimension\n",
        "      n_classes: number of target classes\n",
        "      n_layers: number of layers\n",
        "    \"\"\"\n",
        "\n",
        "    self.n_layers = n_layers\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "\n",
        "    if n_layers == 0:\n",
        "        self.fc = nn.Linear(input_dim, n_classes)\n",
        "    else:\n",
        "        self.convs = nn.ModuleList()\n",
        "\n",
        "        # First layer: input_dim -> hid_dim\n",
        "        self.convs.append(GCNConv(input_dim, hid_dim))\n",
        "\n",
        "        # Subsequent layers: hid_dim * 2 -> hid_dim\n",
        "        for _ in range(n_layers - 1):\n",
        "            self.convs.append(GCNConv(hid_dim, hid_dim))\n",
        "\n",
        "        # Final classification layer\n",
        "        self.out_layer = nn.Linear(hid_dim, n_classes)\n",
        "\n",
        "  def forward(self, X, A) -> torch.Tensor:\n",
        "      \"\"\"\n",
        "      Forward pass for node classification.\n",
        "      \"\"\"\n",
        "      if self.n_layers == 0:\n",
        "          logits = self.fc(X)\n",
        "      else:\n",
        "          h = self.generate_node_embeddings(X, A)\n",
        "          logits = self.out_layer(h)\n",
        "      return logits\n",
        "\n",
        "  def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
        "      \"\"\"\n",
        "      Generate node embeddings with skip connections.\n",
        "      \"\"\"\n",
        "      if self.n_layers == 0:\n",
        "            return self.fc(X)\n",
        "      else:\n",
        "          h = X  # Initial input features\n",
        "          h_prev = None  # Placeholder for skip connections\n",
        "          for i, conv in enumerate(self.convs):\n",
        "              if h_prev is not None:  # Skip connection after the first layer\n",
        "                  h = h + h_prev\n",
        "              h = conv(h, A)  # Apply GCNConv\n",
        "              h_prev = h  # Update skip connection\n",
        "              h = F.relu(h)  # Apply ReLU\n",
        "              h = F.dropout(h, p=self.dropout_ratio, training=self.training)  # Apply dropout\n",
        "          return h\n",
        "\n",
        "  def param_init(self):\n",
        "      \"\"\"\n",
        "      Initialize parameters for the model.\n",
        "      \"\"\"\n",
        "      if self.n_layers == 0:\n",
        "        self.fc.reset_parameters()\n",
        "      else:\n",
        "        for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "        self.out_layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATOoFp0tThMa"
      },
      "source": [
        "### Define GCN with Jumping Knowledge Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DFGJCh53gmvy"
      },
      "outputs": [],
      "source": [
        "class JumpKnowGCN(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_dim: int,\n",
        "      hid_dim: int,\n",
        "      n_classes: int,\n",
        "      n_layers: int,\n",
        "      dropout_ratio: float = 0.3):\n",
        "    super(JumpKnowGCN, self).__init__()\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      input_dim: input feature dimension\n",
        "      hid_dim: hidden feature dimension\n",
        "      n_classes: number of target classes\n",
        "      n_layers: number of layers\n",
        "      dropout_ratio: dropout ratio\n",
        "    \"\"\"\n",
        "    self.n_layers = n_layers\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "\n",
        "    if n_layers == 0:\n",
        "        self.fc = nn.Linear(input_dim, n_classes)\n",
        "    else:\n",
        "        self.convs = nn.ModuleList()\n",
        "\n",
        "        # First layer: input_dim -> hid_dim\n",
        "        self.convs.append(GCNConv(input_dim, hid_dim))\n",
        "\n",
        "        # Additional layers: hid_dim -> hid_dim\n",
        "        for _ in range(n_layers - 1):\n",
        "            self.convs.append(GCNConv(hid_dim, hid_dim))\n",
        "\n",
        "        self.out_layer = nn.Linear(hid_dim, n_classes)\n",
        "\n",
        "  def forward(self, X, A) -> torch.Tensor:\n",
        "        if self.n_layers == 0:\n",
        "            logits = self.fc(X)\n",
        "        else:\n",
        "            h_jk = self.generate_node_embeddings(X, A)\n",
        "            logits = self.out_layer(h_jk)\n",
        "        return logits\n",
        "\n",
        "  def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
        "      if self.n_layers == 0:\n",
        "          return self.fc(X)\n",
        "      else:\n",
        "          h = X\n",
        "          h_list = []\n",
        "          for i, conv in enumerate(self.convs):\n",
        "              h = conv(h, A)\n",
        "              h = F.relu(h)\n",
        "              h = F.dropout(h, p=self.dropout_ratio, training=self.training)\n",
        "              if i > 0 or self.n_layers == 1:  # Skip the first layer unless n_layers == 1\n",
        "                  h_list.append(h)\n",
        "\n",
        "          if len(h_list) > 1:\n",
        "              h_jk = torch.stack(h_list, dim=-1)  # Shape: [num_nodes, hid_dim, n_layers-1]\n",
        "              h_jk, _ = torch.max(h_jk, dim=-1)\n",
        "          else:\n",
        "              h_jk = h_list[0]\n",
        "\n",
        "          return h_jk\n",
        "\n",
        "  def param_init(self):\n",
        "      \"\"\"\n",
        "      Initialize parameters for the model.\n",
        "      \"\"\"\n",
        "      if self.n_layers == 0:\n",
        "        self.fc.reset_parameters()\n",
        "      else:\n",
        "        for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "        self.out_layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erwhnQ8kKPfv"
      },
      "source": [
        "### Define Learnable Weight Shortest Path GCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS0Ga4V9BSGv"
      },
      "source": [
        "#### Define the WSP Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YhJQArGTBnYU"
      },
      "outputs": [],
      "source": [
        "class WSP_GCNConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        output_dim: int,\n",
        "        n_hops: int = 2,\n",
        "        use_residual: bool = False,\n",
        "        learnable_residual_scale: bool = False,\n",
        "        residual_scale_init: float = 0.2\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the Weighted Shortest Path Aggregation module with optional residual connections.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Input feature dimension.\n",
        "            output_dim (int): Output feature dimension.\n",
        "            n_hops (int, optional): Maximum number of hops to consider. Defaults to 2.\n",
        "            use_residual (bool, optional): Whether to include residual connections. Defaults to False.\n",
        "            learnable_residual_scale (bool, optional): Whether the residual scaling factor is learnable. Defaults to False.\n",
        "            residual_scale_init (float, optional): Initial value for residual scaling. Used only if use_residual is True. Defaults to 0.2.\n",
        "        \"\"\"\n",
        "        super(WSP_GCNConv, self).__init__()\n",
        "        self.n_hops = n_hops\n",
        "        self.use_residual = use_residual\n",
        "\n",
        "        # Initialize raw weights (unconstrained)\n",
        "        initial_weights = torch.linspace(-0.3, -1.0, steps=n_hops)\n",
        "        self.raw_weights = nn.Parameter(initial_weights)\n",
        "\n",
        "        # Linear transformation for aggregated features\n",
        "        self.linear = nn.Linear(input_dim, output_dim, bias=False)\n",
        "\n",
        "        # Residual connection: align dimensions if necessary\n",
        "        if self.use_residual:\n",
        "            if input_dim != output_dim:\n",
        "                self.residual = nn.Linear(input_dim, output_dim, bias=False)\n",
        "            else:\n",
        "                self.residual = nn.Identity()\n",
        "\n",
        "            # Residual scaling factor\n",
        "            if learnable_residual_scale:\n",
        "                self.residual_scale = nn.Parameter(torch.tensor(residual_scale_init))\n",
        "            else:\n",
        "                self.register_buffer('residual_scale', torch.tensor(residual_scale_init))\n",
        "        else:\n",
        "            self.residual = None\n",
        "            self.residual_scale = None\n",
        "\n",
        "    def forward(self, h: Tensor, A_norm_d_list: list[Tensor]) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for weighted shortest path aggregation with optional residual connections.\n",
        "\n",
        "        Args:\n",
        "            h (Tensor): Node feature matrix [num_nodes, input_dim].\n",
        "            A_norm_d_list (list[Tensor]): List of normalized adjacency matrices for each hop.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Aggregated node features [num_nodes, output_dim].\n",
        "        \"\"\"\n",
        "        # Apply sigmoid to raw_weights to get normalized weights\n",
        "        w = torch.sigmoid(self.raw_weights)  # Shape: [n_hops]\n",
        "\n",
        "        # Initialize aggregation tensor\n",
        "        agg = torch.zeros_like(h)\n",
        "\n",
        "        # Iterate only over the required number of hops\n",
        "        for i, A_norm_d in enumerate(A_norm_d_list[:self.n_hops], start=1):\n",
        "            agg += w[i-1] * torch.mm(A_norm_d, h)\n",
        "\n",
        "        # Apply linear transformation to aggregated features\n",
        "        agg_transformed = self.linear(agg)\n",
        "\n",
        "        if self.use_residual:\n",
        "            residual = self.residual(h)\n",
        "            out = agg_transformed + self.residual_scale * residual\n",
        "        else:\n",
        "            out = agg_transformed\n",
        "\n",
        "        return out\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Reset all learnable parameters in the module and possibly add random noise to raw_weights and residual_scale.\n",
        "        \"\"\"\n",
        "        # Reset raw_weights to initial values\n",
        "        self.raw_weights.data = torch.linspace(-0.3, -1.0, steps=self.n_hops).to(self.raw_weights.device)\n",
        "\n",
        "        noise_std = 0.005 # Standard deviation of the noise\n",
        "        self.raw_weights.data += torch.randn_like(self.raw_weights) * noise_std # Add small random noise to raw_weights\n",
        "\n",
        "        # Reset linear transformation\n",
        "        self.linear.reset_parameters()\n",
        "\n",
        "        if self.use_residual:\n",
        "            if isinstance(self.residual, nn.Linear):\n",
        "                self.residual.reset_parameters()\n",
        "\n",
        "            # Reset residual scaling\n",
        "            if isinstance(self.residual_scale, nn.Parameter):\n",
        "                nn.init.constant_(self.residual_scale, 0.2)\n",
        "                # Add small random noise to residual_scale\n",
        "                self.residual_scale.data += torch.randn_like(self.residual_scale) * noise_std\n",
        "            else:\n",
        "                self.residual_scale.data = torch.tensor(0.75).to(self.residual_scale.device)\n",
        "                # Add small random noise to residual_scale\n",
        "                self.residual_scale.data += torch.randn_like(self.residual_scale) * noise_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPHWdIMSCR7w"
      },
      "source": [
        "#### Integrate WSP_GCNConv into a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k6K_3AzDCRb6"
      },
      "outputs": [],
      "source": [
        "class WSP_GCN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hid_dim: int,\n",
        "        n_classes: int,\n",
        "        n_layers: int = 2,\n",
        "        n_hops: int = 2,\n",
        "        dropout_ratio: float = 0.3,\n",
        "        use_residual: bool = False,\n",
        "        learnable_residual_scale: bool = False\n",
        "    ):\n",
        "        super(WSP_GCN, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): Input feature dimension.\n",
        "            hid_dim (int): Hidden feature dimension.\n",
        "            n_classes (int): Number of target classes.\n",
        "            n_layers (int, optional): Number of layers. Defaults to 2.\n",
        "            n_hops (int, optional): Maximum number of hops to consider. Defaults to 2.\n",
        "            dropout_ratio (float, optional): Dropout ratio. Defaults to 0.3.\n",
        "            use_residual (bool, optional): Whether to include residual connections. Defaults to False.\n",
        "            learnable_residual_scale (bool, optional): Whether residual scaling is learnable. Defaults to False.\n",
        "        \"\"\"\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.n_hops = n_hops\n",
        "        self.use_residual = use_residual\n",
        "        self.learnable_residual_scale = learnable_residual_scale\n",
        "\n",
        "        # Define layers\n",
        "        if n_layers == 0:\n",
        "            self.fc = nn.Linear(input_dim, n_classes)\n",
        "        else:\n",
        "            self.convs = nn.ModuleList()\n",
        "            # First layer: input_dim -> hid_dim\n",
        "            self.convs.append(\n",
        "                WSP_GCNConv(\n",
        "                    input_dim,\n",
        "                    hid_dim,\n",
        "                    n_hops=n_hops,\n",
        "                    use_residual=use_residual,\n",
        "                    learnable_residual_scale=learnable_residual_scale\n",
        "                )\n",
        "            )\n",
        "            # Subsequent layers: hid_dim -> hid_dim\n",
        "            for _ in range(n_layers - 1):\n",
        "                self.convs.append(\n",
        "                    WSP_GCNConv(\n",
        "                        hid_dim,\n",
        "                        hid_dim,\n",
        "                        n_hops=n_hops,\n",
        "                        use_residual=use_residual,\n",
        "                        learnable_residual_scale=learnable_residual_scale\n",
        "                    )\n",
        "                )\n",
        "            # Final classification layer\n",
        "            self.out_layer = nn.Linear(hid_dim, n_classes)\n",
        "\n",
        "    def forward(self, X, A_norm_d_list: list[Tensor]) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the classification task.\n",
        "\n",
        "        Args:\n",
        "            X (torch.Tensor): Node feature matrix [num_nodes, input_dim].\n",
        "            A_norm_d_list (list[torch.Tensor]): List of normalized adjacency matrices for each hop.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits for each node [num_nodes, n_classes].\n",
        "        \"\"\"\n",
        "        if self.n_layers == 0:\n",
        "            return self.fc(X)\n",
        "        else:\n",
        "            h = self.generate_node_embeddings(X, A_norm_d_list)\n",
        "            logits = self.out_layer(h)  # Final layer for classification\n",
        "            return logits\n",
        "\n",
        "    def generate_node_embeddings(self, X, A_norm_d_list: list[Tensor]) -> Tensor:\n",
        "        \"\"\"\n",
        "        Generate node embeddings by passing input through convolutional layers.\n",
        "\n",
        "        Args:\n",
        "            X (torch.Tensor): Node feature matrix [num_nodes, input_dim].\n",
        "            A_norm_d_list (list[torch.Tensor]): List of normalized adjacency matrices for each hop.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Node embeddings [num_nodes, hid_dim].\n",
        "        \"\"\"\n",
        "        if self.n_layers == 0:\n",
        "            return self.fc(X)\n",
        "        else:\n",
        "            h = X\n",
        "            for conv in self.convs:\n",
        "                h = conv(h, A_norm_d_list)  # Apply WSP_GCNConv\n",
        "                h = F.relu(h)  # Apply ReLU activation\n",
        "                h = F.dropout(h, p=self.dropout_ratio, training=self.training)  # Apply dropout\n",
        "            return h\n",
        "\n",
        "    def param_init(self):\n",
        "        \"\"\"\n",
        "        Initialize parameters for the model.\n",
        "        \"\"\"\n",
        "        if self.n_layers == 0:\n",
        "            self.fc.reset_parameters()\n",
        "        else:\n",
        "            for conv in self.convs:\n",
        "                conv.reset_parameters()\n",
        "            self.out_layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XZL4Psp1BIs"
      },
      "source": [
        "## Define Train and Test Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup Adjacency Matrix for WSP-GCN"
      ],
      "metadata": {
        "id": "Aa6F0zzFe8NU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JA0EUyCTrESg"
      },
      "outputs": [],
      "source": [
        "adj_matrix = to_dense_adj(dataset.data.edge_index).squeeze(0).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_normalized_adjacency(adj: Tensor, n_hops: int) -> list[Tensor]:\n",
        "    \"\"\"\n",
        "    Computes normalized adjacency matrices for each hop up to n_hops.\n",
        "\n",
        "    Args:\n",
        "        adj (torch.Tensor): Adjacency matrix [num_nodes, num_nodes].\n",
        "        n_hops (int): Maximum number of hops.\n",
        "\n",
        "    Returns:\n",
        "        list[torch.Tensor]: List of normalized adjacency matrices for each hop.\n",
        "    \"\"\"\n",
        "    A_power_list = [adj.clone()]\n",
        "\n",
        "    # Compute A^d for d = 2,...,n_hops\n",
        "    for d in range(2, n_hops + 1):\n",
        "        A_power = torch.mm(A_power_list[-1], adj)\n",
        "        A_power_list.append(A_power.clone())\n",
        "\n",
        "    exact_hop_adj = []\n",
        "    for d in range(1, n_hops + 1):\n",
        "        if d == 1:\n",
        "            A_d = A_power_list[0].clone()\n",
        "            A_d += torch.eye(A_d.size(0), device=A_d.device)  # Add self-loops for 1-hop\n",
        "        else:\n",
        "            A_d = (A_power_list[d-1] > 0).float()\n",
        "            for i in range(1, d):\n",
        "                A_d = A_d - A_power_list[i-1]\n",
        "            A_d = (A_d > 0).float()\n",
        "            A_d.fill_diagonal_(0)  # Remove self-loops for i >= 2\n",
        "        exact_hop_adj.append(A_d)\n",
        "\n",
        "    # Normalize each A_d\n",
        "    A_norm_d_list = []\n",
        "    for A_d in exact_hop_adj:\n",
        "        degree_i = A_d.sum(dim=1)  # Degree vector\n",
        "        degree_i = torch.where(degree_i > 0, degree_i, torch.ones_like(degree_i))  # Avoid division by zero\n",
        "        D_i_inv_sqrt = torch.pow(degree_i, -0.5)\n",
        "        D_i_inv_sqrt_matrix = torch.diag(D_i_inv_sqrt)\n",
        "        A_norm_d = torch.mm(torch.mm(D_i_inv_sqrt_matrix, A_d), D_i_inv_sqrt_matrix)\n",
        "        A_norm_d_list.append(A_norm_d)\n",
        "\n",
        "    return A_norm_d_list"
      ],
      "metadata": {
        "id": "Ts3ra5-HVWs9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute normalized adjacency matrices once\n",
        "A_norm_d_list = compute_normalized_adjacency(adj_matrix, n_hops=10)  # Example with n_hops=4"
      ],
      "metadata": {
        "id": "2NIkutMhVtM5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train Function"
      ],
      "metadata": {
        "id": "J-xJ8Z_dfEz4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ruiSxxk71F1O"
      },
      "outputs": [],
      "source": [
        "def train(params: typing.Dict, A_norm_d_list: list[Tensor]) -> torch.nn.Module:\n",
        "    \"\"\"\n",
        "    This function trains a node classification model and returns the trained model object.\n",
        "    It also logs validation accuracies, raw_weights, and residual_scale (for WSP_GCN) for visualization purposes.\n",
        "    \"\"\"\n",
        "    global results  # Declare results as global to modify it within the function\n",
        "\n",
        "    # Update parameters\n",
        "    params[\"n_classes\"] = dataset.num_classes  # Number of target classes\n",
        "    params[\"input_dim\"] = dataset.num_features  # Size of input features\n",
        "\n",
        "    # Set a model\n",
        "    if params['model_name'] == 'GCN':\n",
        "        model = GCN(\n",
        "            params[\"input_dim\"],\n",
        "            params[\"hid_dim\"],\n",
        "            params[\"n_classes\"],\n",
        "            params[\"n_layers\"]\n",
        "        ).to(device)\n",
        "    elif params['model_name'] == 'SkipGCN':\n",
        "        model = SkipGCN(\n",
        "            params[\"input_dim\"],\n",
        "            params[\"hid_dim\"],\n",
        "            params[\"n_classes\"],\n",
        "            params[\"n_layers\"]\n",
        "        ).to(device)\n",
        "    elif params['model_name'] == 'JumpKnowGCN':\n",
        "        model = JumpKnowGCN(\n",
        "            params[\"input_dim\"],\n",
        "            params[\"hid_dim\"],\n",
        "            params[\"n_classes\"],\n",
        "            params[\"n_layers\"]\n",
        "        ).to(device)\n",
        "    elif params['model_name'].startswith('WSP_GCN'):\n",
        "        # Extract residual parameters\n",
        "        use_residual = params.get(\"use_residual\", False)\n",
        "        learnable_residual_scale = params.get(\"learnable_residual_scale\", False)\n",
        "        num_hops = params.get(\"num_hops\", 2)\n",
        "\n",
        "        model = WSP_GCN(\n",
        "            params[\"input_dim\"],\n",
        "            params[\"hid_dim\"],\n",
        "            params[\"n_classes\"],\n",
        "            params[\"n_layers\"],\n",
        "            n_hops=num_hops,\n",
        "            dropout_ratio=params.get(\"dropout_ratio\", 0.3),\n",
        "            use_residual=use_residual,\n",
        "            learnable_residual_scale=learnable_residual_scale\n",
        "        ).to(device)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Model {params['model_name']} is not implemented.\")\n",
        "\n",
        "    # Initialize parameters\n",
        "    model.param_init()\n",
        "\n",
        "    # Define optimizer and loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Initialize storage for validation accuracies and weights\n",
        "    val_accuracies = []\n",
        "    weights_over_epochs = {}\n",
        "\n",
        "    for epoch in range(params[\"epochs\"]):\n",
        "        model.train()  # Set the model to training mode\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        if params['model_name'].startswith('WSP_GCN'):\n",
        "            logits = model(data.x, A_norm_d_list)  # Use precomputed normalized adjacencies\n",
        "        else:\n",
        "            logits = model(data.x, data.edge_index)  # Use edge index\n",
        "        loss = loss_fn(logits[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        if params['model_name'].startswith('WSP_GCN'):\n",
        "            val_accuracy = evaluate(model, data, data.val_mask, A_norm_d_list)\n",
        "        else:\n",
        "            val_accuracy = evaluate(model, data, data.val_mask)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Track weights and residual scales for WSP_GCN\n",
        "        if params['model_name'].startswith('WSP_GCN'):\n",
        "            weights_over_epochs[epoch] = {}\n",
        "            for layer_idx, layer in enumerate(model.convs):\n",
        "                # Store raw_weights\n",
        "                weights_over_epochs[epoch][layer_idx] = {\n",
        "                    \"raw_weights\": layer.raw_weights.clone().detach().cpu().tolist()\n",
        "                }\n",
        "                # Store residual_scale if residual connections are used\n",
        "                if layer.use_residual and layer.residual_scale is not None:\n",
        "                    weights_over_epochs[epoch][layer_idx][\"residual_scale\"] = layer.residual_scale.clone().detach().cpu().item()\n",
        "                else:\n",
        "                    weights_over_epochs[epoch][layer_idx][\"residual_scale\"] = None  # Or omit this key if preferred\n",
        "\n",
        "        # Print every 20 epochs\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            # if params['model_name'].startswith('WSP_GCN'):\n",
        "            #     for idx, conv in enumerate(model.convs):\n",
        "            #         print(f\"Epoch {epoch+1}, Layer {idx+1} raw_weights: {conv.raw_weights.data}\")\n",
        "            #         if conv.use_residual:\n",
        "            #             print(f\"Epoch {epoch+1}, Layer {idx+1} residual_scale: {conv.residual_scale.data}\")\n",
        "            print(f\"Epoch {epoch + 1}/{params['epochs']}, Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Save validation accuracies and weights\n",
        "    run = params.get(\"run\", None)  # Get 'run' from params\n",
        "    if run is not None:\n",
        "        results[params[\"model_name\"]][params[\"n_layers\"]][run][\"val_accuracies\"] = val_accuracies\n",
        "        if params['model_name'].startswith('WSP_GCN'):\n",
        "            results[params[\"model_name\"]][params[\"n_layers\"]][run][\"num_hops\"] = params.get(\"num_hops\", \"N/A\")  # Add num_hops\n",
        "            results[params[\"model_name\"]][params[\"n_layers\"]][run][\"weights_over_epochs\"] = weights_over_epochs\n",
        "    else:\n",
        "        print(\"Warning: 'run' parameter not found in params. Validation accuracies and weights not saved.\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation Code"
      ],
      "metadata": {
        "id": "c1uhJudZfIT-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oTIlBJHL1Hk3"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model,\n",
        "    data,\n",
        "    mask,\n",
        "    A_norm_d_list: list[Tensor] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given mask.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        data (torch_geometric.data.Data): The graph data.\n",
        "        mask (torch.Tensor): The mask indicating the nodes to evaluate.\n",
        "        A_norm_d_list (Optional[list[Tensor]]): Precomputed normalized adjacency matrices for WSP_GCN.\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy percentage.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        # Use precomputed normalized adjacency if provided (WSP_GCN)\n",
        "        if A_norm_d_list is not None:\n",
        "            out = model(data.x, A_norm_d_list)  # Forward pass with precomputed adjacencies\n",
        "        else:\n",
        "            out = model(data.x, data.edge_index)  # Forward pass with edge index\n",
        "        predictions = out.argmax(dim=1)  # Get predicted class for each node\n",
        "        correct = (predictions[mask] == data.y[mask]).sum().item()  # Count correct predictions\n",
        "        total = mask.sum().item()  # Total number of nodes in the mask\n",
        "        accuracy = correct / total  # Calculate accuracy\n",
        "\n",
        "    return accuracy * 100  # Return accuracy as a percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfjaNBSEYPln"
      },
      "source": [
        "#### Testing Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPOoAzACKYmK",
        "outputId": "101b1151-c7e9-4315-b1b7-8de92490db22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/80, Loss: 1.6383, Validation Accuracy: 32.6000\n",
            "Epoch 40/80, Loss: 1.5265, Validation Accuracy: 23.2000\n",
            "Epoch 60/80, Loss: 1.5043, Validation Accuracy: 26.2000\n",
            "Epoch 80/80, Loss: 1.0622, Validation Accuracy: 30.0000\n",
            "Warning: 'run' parameter not found in params. Validation accuracies and weights not saved.\n"
          ]
        }
      ],
      "source": [
        "training_params = {\n",
        "    \"lr\": 0.005,  # learning rate\n",
        "    \"weight_decay\": 0.0005,  # weight_decay\n",
        "    \"epochs\": 80,  # number of total training epochs\n",
        "    \"hid_dim\": 64, # size of hidden features\n",
        "    \"n_layers\": 10, # number of layers\n",
        "    \"model_name\": \"GCN\"\n",
        "}\n",
        "training_params[\"n_layers\"] = 10\n",
        "model_1_layer = train(training_params, A_norm_d_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3ahYLbxKear",
        "outputId": "c22e75ab-acb7-42d3-a217-74e91e2bf283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/80, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 40/80, Loss: 1.9459, Validation Accuracy: 15.6000\n",
            "Epoch 60/80, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Epoch 80/80, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Warning: 'run' parameter not found in params. Validation accuracies and weights not saved.\n"
          ]
        }
      ],
      "source": [
        "training_params[\"model_name\"] = 'WSP_GCN'\n",
        "training_params[\"n_layers\"] = 10\n",
        "training_params[\"num_hops\"] = 1\n",
        "training_params[\"use_residual\"] = False\n",
        "training_params[\"learnable_residual_scale\"] = True\n",
        "model_2_hops = train(training_params, A_norm_d_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_params[\"model_name\"] = 'WSP_GCN'\n",
        "training_params[\"n_layers\"] = 10\n",
        "training_params[\"num_hops\"] = 4\n",
        "training_params[\"use_residual\"] = True\n",
        "training_params[\"learnable_residual_scale\"] = False\n",
        "model_2_hops = train(training_params, A_norm_d_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3K5GCQcof43",
        "outputId": "7849b2b9-8647-4ed8-8084-f3e64345a1ae"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/80, Loss: 1.4911, Validation Accuracy: 39.8000\n",
            "Epoch 40/80, Loss: 1.0781, Validation Accuracy: 65.2000\n",
            "Epoch 60/80, Loss: 0.7600, Validation Accuracy: 66.4000\n",
            "Epoch 80/80, Loss: 0.6564, Validation Accuracy: 69.6000\n",
            "Warning: 'run' parameter not found in params. Validation accuracies and weights not saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_params[\"model_name\"] = 'JumpKnowGCN'\n",
        "training_params[\"n_layers\"] = 10\n",
        "training_params[\"num_hops\"] = 4\n",
        "model_2_hops = train(training_params, A_norm_d_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2CFiNbS1DY2",
        "outputId": "38da3887-a7e8-4e2c-beba-26102fe916e0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/80, Loss: 0.1153, Validation Accuracy: 81.6000\n",
            "Epoch 40/80, Loss: 0.0142, Validation Accuracy: 73.8000\n",
            "Epoch 60/80, Loss: 0.0033, Validation Accuracy: 75.6000\n",
            "Epoch 80/80, Loss: 0.0032, Validation Accuracy: 76.8000\n",
            "Warning: 'run' parameter not found in params. Validation accuracies and weights not saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_params[\"model_name\"] = 'WSP_GCN'\n",
        "training_params[\"n_layers\"] = 1\n",
        "training_params[\"num_hops\"] = 3\n",
        "model_2_hops = train(training_params, A_norm_d_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIWYMm2EtCpe",
        "outputId": "093ff6fa-0bdc-49fd-c314-2291920c37f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/80, Loss: 0.0705, Validation Accuracy: 75.6000\n",
            "Epoch 40/80, Loss: 0.0081, Validation Accuracy: 74.4000\n",
            "Epoch 60/80, Loss: 0.0093, Validation Accuracy: 73.8000\n",
            "Epoch 80/80, Loss: 0.0106, Validation Accuracy: 74.4000\n",
            "Warning: 'run' parameter not found in params. Validation accuracies and weights not saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_params[\"model_name\"] = 'WSP_GCN'\n",
        "training_params[\"n_layers\"] = 1\n",
        "training_params[\"num_hops\"] = 4\n",
        "model_2_hops = train(training_params, A_norm_d_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSdxeGU2vqYv",
        "outputId": "23771fe2-94ec-4481-d722-0efc37e908ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/80, Loss: 0.0503, Validation Accuracy: 77.4000\n",
            "Epoch 40/80, Loss: 0.0090, Validation Accuracy: 76.4000\n",
            "Epoch 60/80, Loss: 0.0071, Validation Accuracy: 75.8000\n",
            "Epoch 80/80, Loss: 0.0113, Validation Accuracy: 75.0000\n",
            "Warning: 'run' parameter not found in params. Validation accuracies and weights not saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZUcdePSDEgJ"
      },
      "source": [
        "## Gather Data for Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoCgmkqfIyj3"
      },
      "source": [
        "#### Setup Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2SRLh9EQDGtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ed7b8a-b0ec-4089-f055-5b72bc5a68f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x794e1de638d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Define the list of models and their specific parameters\n",
        "models = {\n",
        "    \"WSP_GCN_1hop_no_residual\": {\n",
        "        \"num_hops\": 1,\n",
        "        \"use_residual\": False,\n",
        "        \"learnable_residual_scale\": False\n",
        "    },\n",
        "    \"WSP_GCN_2hop_no_residual\": {\n",
        "        \"num_hops\": 2,\n",
        "        \"use_residual\": False,\n",
        "        \"learnable_residual_scale\": False\n",
        "    },\n",
        "    \"WSP_GCN_4hop_no_residual\": {\n",
        "        \"num_hops\": 4,\n",
        "        \"use_residual\": False,\n",
        "        \"learnable_residual_scale\": False\n",
        "    },\n",
        "    \"WSP_GCN_2hop_residual_fixed\": {\n",
        "        \"num_hops\": 2,\n",
        "        \"use_residual\": True,\n",
        "        \"learnable_residual_scale\": False\n",
        "    },\n",
        "    \"WSP_GCN_2hop_residual_learnable\": {\n",
        "        \"num_hops\": 2,\n",
        "        \"use_residual\": True,\n",
        "        \"learnable_residual_scale\": True\n",
        "    },\n",
        "    \"WSP_GCN_4hop_residual_fixed\": {\n",
        "        \"num_hops\": 4,\n",
        "        \"use_residual\": True,\n",
        "        \"learnable_residual_scale\": False\n",
        "    },\n",
        "    \"WSP_GCN_4hop_residual_learnable\": {\n",
        "        \"num_hops\": 4,\n",
        "        \"use_residual\": True,\n",
        "        \"learnable_residual_scale\": True\n",
        "    },\n",
        "    \"GCN\": {\n",
        "        \"num_hops\": None  # Not applicable\n",
        "    },\n",
        "    \"SkipGCN\": {\n",
        "        \"num_hops\": None  # Not applicable\n",
        "    },\n",
        "    \"JumpKnowGCN\": {\n",
        "        \"num_hops\": None  # Not applicable\n",
        "    }\n",
        "}\n",
        "\n",
        "# Define the number of layers to test\n",
        "layer_counts = [1, 3, 8, 10]\n",
        "\n",
        "# Number of runs per configuration\n",
        "num_runs = 4\n",
        "\n",
        "# Define other training parameters\n",
        "base_training_params = {\n",
        "    \"lr\": 0.005,  # Learning rate\n",
        "    \"weight_decay\": 0.0005,  # Weight decay\n",
        "    \"epochs\": 100,  # Number of training epochs\n",
        "    \"hid_dim\": 64,  # Hidden dimension\n",
        "    \"dropout_ratio\": 0.3  # Dropout ratio\n",
        "}\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9u2hydV1XqY"
      },
      "source": [
        "### Experiment Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9tEO5w8K-TE",
        "outputId": "76fe6685-b1b6-4169-f415-5314afa4bea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 1, Run: 1\n",
            "Epoch 20/100, Loss: 0.7215, Validation Accuracy: 75.2000\n",
            "Epoch 40/100, Loss: 0.0871, Validation Accuracy: 75.4000\n",
            "Epoch 60/100, Loss: 0.0396, Validation Accuracy: 75.2000\n",
            "Epoch 80/100, Loss: 0.0386, Validation Accuracy: 75.6000\n",
            "Epoch 100/100, Loss: 0.0359, Validation Accuracy: 75.0000\n",
            "Run 1 Test Accuracy: 77.50%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 1, Run: 2\n",
            "Epoch 20/100, Loss: 0.7389, Validation Accuracy: 74.8000\n",
            "Epoch 40/100, Loss: 0.0808, Validation Accuracy: 75.8000\n",
            "Epoch 60/100, Loss: 0.0365, Validation Accuracy: 76.0000\n",
            "Epoch 80/100, Loss: 0.0385, Validation Accuracy: 76.0000\n",
            "Epoch 100/100, Loss: 0.0380, Validation Accuracy: 75.0000\n",
            "Run 2 Test Accuracy: 76.90%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 1, Run: 3\n",
            "Epoch 20/100, Loss: 0.7392, Validation Accuracy: 72.0000\n",
            "Epoch 40/100, Loss: 0.0849, Validation Accuracy: 76.0000\n",
            "Epoch 60/100, Loss: 0.0396, Validation Accuracy: 76.6000\n",
            "Epoch 80/100, Loss: 0.0444, Validation Accuracy: 76.2000\n",
            "Epoch 100/100, Loss: 0.0381, Validation Accuracy: 75.4000\n",
            "Run 3 Test Accuracy: 78.10%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 1, Run: 4\n",
            "Epoch 20/100, Loss: 0.7543, Validation Accuracy: 73.4000\n",
            "Epoch 40/100, Loss: 0.0807, Validation Accuracy: 74.8000\n",
            "Epoch 60/100, Loss: 0.0358, Validation Accuracy: 75.2000\n",
            "Epoch 80/100, Loss: 0.0380, Validation Accuracy: 75.4000\n",
            "Epoch 100/100, Loss: 0.0349, Validation Accuracy: 74.2000\n",
            "Run 4 Test Accuracy: 77.80%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 3, Run: 1\n",
            "Epoch 20/100, Loss: 1.7104, Validation Accuracy: 12.0000\n",
            "Epoch 40/100, Loss: 0.3765, Validation Accuracy: 63.6000\n",
            "Epoch 60/100, Loss: 0.1010, Validation Accuracy: 67.6000\n",
            "Epoch 80/100, Loss: 0.0430, Validation Accuracy: 68.0000\n",
            "Epoch 100/100, Loss: 0.0270, Validation Accuracy: 68.4000\n",
            "Run 1 Test Accuracy: 67.90%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 3, Run: 2\n",
            "Epoch 20/100, Loss: 1.8151, Validation Accuracy: 24.8000\n",
            "Epoch 40/100, Loss: 0.3235, Validation Accuracy: 75.0000\n",
            "Epoch 60/100, Loss: 0.0377, Validation Accuracy: 72.6000\n",
            "Epoch 80/100, Loss: 0.0221, Validation Accuracy: 75.6000\n",
            "Epoch 100/100, Loss: 0.0195, Validation Accuracy: 72.0000\n",
            "Run 2 Test Accuracy: 75.20%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 3, Run: 3\n",
            "Epoch 20/100, Loss: 1.7304, Validation Accuracy: 18.2000\n",
            "Epoch 40/100, Loss: 0.3516, Validation Accuracy: 63.6000\n",
            "Epoch 60/100, Loss: 0.0418, Validation Accuracy: 69.0000\n",
            "Epoch 80/100, Loss: 0.0306, Validation Accuracy: 69.8000\n",
            "Epoch 100/100, Loss: 0.0238, Validation Accuracy: 68.0000\n",
            "Run 3 Test Accuracy: 70.50%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 3, Run: 4\n",
            "Epoch 20/100, Loss: 1.8819, Validation Accuracy: 27.4000\n",
            "Epoch 40/100, Loss: 0.4891, Validation Accuracy: 75.4000\n",
            "Epoch 60/100, Loss: 0.0418, Validation Accuracy: 73.8000\n",
            "Epoch 80/100, Loss: 0.0237, Validation Accuracy: 71.8000\n",
            "Epoch 100/100, Loss: 0.0254, Validation Accuracy: 73.4000\n",
            "Run 4 Test Accuracy: 73.70%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 8, Run: 1\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 16.2000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 16.2000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 15.6000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 16.2000\n",
            "Run 1 Test Accuracy: 14.90%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 8, Run: 2\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 7.2000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 31.6000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 31.6000\n",
            "Run 2 Test Accuracy: 31.90%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 8, Run: 3\n",
            "Epoch 20/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Run 3 Test Accuracy: 13.00%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 8, Run: 4\n",
            "Epoch 20/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 16.2000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 16.2000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Run 4 Test Accuracy: 10.30%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 10, Run: 1\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 12.2000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 15.6000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Run 1 Test Accuracy: 9.10%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 10, Run: 2\n",
            "Epoch 20/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 31.6000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 16.2000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 31.6000\n",
            "Run 2 Test Accuracy: 31.90%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 10, Run: 3\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 31.6000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 16.2000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Run 3 Test Accuracy: 6.40%\n",
            "\n",
            "Running Model: WSP_GCN_1hop_no_residual, Layers: 10, Run: 4\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 12.2000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 31.6000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Run 4 Test Accuracy: 6.40%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 1, Run: 1\n",
            "Epoch 20/100, Loss: 0.5688, Validation Accuracy: 77.2000\n",
            "Epoch 40/100, Loss: 0.0737, Validation Accuracy: 77.6000\n",
            "Epoch 60/100, Loss: 0.0263, Validation Accuracy: 78.0000\n",
            "Epoch 80/100, Loss: 0.0337, Validation Accuracy: 77.8000\n",
            "Epoch 100/100, Loss: 0.0325, Validation Accuracy: 77.0000\n",
            "Run 1 Test Accuracy: 80.40%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 1, Run: 2\n",
            "Epoch 20/100, Loss: 0.5399, Validation Accuracy: 77.0000\n",
            "Epoch 40/100, Loss: 0.0679, Validation Accuracy: 76.8000\n",
            "Epoch 60/100, Loss: 0.0448, Validation Accuracy: 77.2000\n",
            "Epoch 80/100, Loss: 0.0351, Validation Accuracy: 77.4000\n",
            "Epoch 100/100, Loss: 0.0310, Validation Accuracy: 77.4000\n",
            "Run 2 Test Accuracy: 80.90%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 1, Run: 3\n",
            "Epoch 20/100, Loss: 0.5180, Validation Accuracy: 76.8000\n",
            "Epoch 40/100, Loss: 0.0706, Validation Accuracy: 76.8000\n",
            "Epoch 60/100, Loss: 0.0308, Validation Accuracy: 77.6000\n",
            "Epoch 80/100, Loss: 0.0368, Validation Accuracy: 76.8000\n",
            "Epoch 100/100, Loss: 0.0281, Validation Accuracy: 77.2000\n",
            "Run 3 Test Accuracy: 81.70%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 1, Run: 4\n",
            "Epoch 20/100, Loss: 0.5580, Validation Accuracy: 78.4000\n",
            "Epoch 40/100, Loss: 0.0519, Validation Accuracy: 77.8000\n",
            "Epoch 60/100, Loss: 0.0274, Validation Accuracy: 77.6000\n",
            "Epoch 80/100, Loss: 0.0372, Validation Accuracy: 77.4000\n",
            "Epoch 100/100, Loss: 0.0333, Validation Accuracy: 77.0000\n",
            "Run 4 Test Accuracy: 80.90%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 3, Run: 1\n",
            "Epoch 20/100, Loss: 1.1484, Validation Accuracy: 55.2000\n",
            "Epoch 40/100, Loss: 0.2446, Validation Accuracy: 71.6000\n",
            "Epoch 60/100, Loss: 0.0473, Validation Accuracy: 72.4000\n",
            "Epoch 80/100, Loss: 0.0276, Validation Accuracy: 72.0000\n",
            "Epoch 100/100, Loss: 0.0286, Validation Accuracy: 72.4000\n",
            "Run 1 Test Accuracy: 74.70%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 3, Run: 2\n",
            "Epoch 20/100, Loss: 1.1838, Validation Accuracy: 60.4000\n",
            "Epoch 40/100, Loss: 0.1122, Validation Accuracy: 77.8000\n",
            "Epoch 60/100, Loss: 0.0341, Validation Accuracy: 75.8000\n",
            "Epoch 80/100, Loss: 0.0290, Validation Accuracy: 73.8000\n",
            "Epoch 100/100, Loss: 0.0402, Validation Accuracy: 75.6000\n",
            "Run 2 Test Accuracy: 75.00%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 3, Run: 3\n",
            "Epoch 20/100, Loss: 1.1507, Validation Accuracy: 50.6000\n",
            "Epoch 40/100, Loss: 0.1514, Validation Accuracy: 75.4000\n",
            "Epoch 60/100, Loss: 0.0364, Validation Accuracy: 76.0000\n",
            "Epoch 80/100, Loss: 0.0441, Validation Accuracy: 73.8000\n",
            "Epoch 100/100, Loss: 0.0183, Validation Accuracy: 75.0000\n",
            "Run 3 Test Accuracy: 75.00%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 3, Run: 4\n",
            "Epoch 20/100, Loss: 1.1522, Validation Accuracy: 46.0000\n",
            "Epoch 40/100, Loss: 0.1324, Validation Accuracy: 76.6000\n",
            "Epoch 60/100, Loss: 0.0399, Validation Accuracy: 74.2000\n",
            "Epoch 80/100, Loss: 0.0384, Validation Accuracy: 75.8000\n",
            "Epoch 100/100, Loss: 0.0303, Validation Accuracy: 75.0000\n",
            "Run 4 Test Accuracy: 76.30%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 8, Run: 1\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 7.2000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Run 1 Test Accuracy: 13.00%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 8, Run: 2\n",
            "Epoch 20/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 31.6000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 31.6000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Run 2 Test Accuracy: 13.00%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 8, Run: 3\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 7.2000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 31.6000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 31.6000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Run 3 Test Accuracy: 9.10%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 8, Run: 4\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 12.2000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Run 4 Test Accuracy: 9.10%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 10, Run: 1\n",
            "Epoch 20/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 16.2000\n",
            "Run 1 Test Accuracy: 14.90%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 10, Run: 2\n",
            "Epoch 20/100, Loss: 1.9459, Validation Accuracy: 15.6000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 15.6000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 16.2000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 15.6000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 16.2000\n",
            "Run 2 Test Accuracy: 14.90%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 10, Run: 3\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 15.6000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Run 3 Test Accuracy: 13.00%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_no_residual, Layers: 10, Run: 4\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 15.6000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 15.6000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 16.2000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Run 4 Test Accuracy: 9.10%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 1, Run: 1\n",
            "Epoch 20/100, Loss: 0.4076, Validation Accuracy: 78.0000\n",
            "Epoch 40/100, Loss: 0.0789, Validation Accuracy: 77.6000\n",
            "Epoch 60/100, Loss: 0.0286, Validation Accuracy: 77.8000\n",
            "Epoch 80/100, Loss: 0.0278, Validation Accuracy: 78.0000\n",
            "Epoch 100/100, Loss: 0.0286, Validation Accuracy: 78.2000\n",
            "Run 1 Test Accuracy: 79.90%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 1, Run: 2\n",
            "Epoch 20/100, Loss: 0.4351, Validation Accuracy: 78.8000\n",
            "Epoch 40/100, Loss: 0.0814, Validation Accuracy: 76.8000\n",
            "Epoch 60/100, Loss: 0.0468, Validation Accuracy: 76.8000\n",
            "Epoch 80/100, Loss: 0.0374, Validation Accuracy: 78.4000\n",
            "Epoch 100/100, Loss: 0.0256, Validation Accuracy: 78.0000\n",
            "Run 2 Test Accuracy: 80.10%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 1, Run: 3\n",
            "Epoch 20/100, Loss: 0.4091, Validation Accuracy: 78.6000\n",
            "Epoch 40/100, Loss: 0.0613, Validation Accuracy: 76.6000\n",
            "Epoch 60/100, Loss: 0.0386, Validation Accuracy: 77.2000\n",
            "Epoch 80/100, Loss: 0.0268, Validation Accuracy: 78.0000\n",
            "Epoch 100/100, Loss: 0.0287, Validation Accuracy: 78.6000\n",
            "Run 3 Test Accuracy: 79.90%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 1, Run: 4\n",
            "Epoch 20/100, Loss: 0.4348, Validation Accuracy: 78.0000\n",
            "Epoch 40/100, Loss: 0.0822, Validation Accuracy: 76.6000\n",
            "Epoch 60/100, Loss: 0.0385, Validation Accuracy: 77.8000\n",
            "Epoch 80/100, Loss: 0.0231, Validation Accuracy: 77.4000\n",
            "Epoch 100/100, Loss: 0.0366, Validation Accuracy: 78.0000\n",
            "Run 4 Test Accuracy: 80.10%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 3, Run: 1\n",
            "Epoch 20/100, Loss: 0.9456, Validation Accuracy: 68.8000\n",
            "Epoch 40/100, Loss: 0.4917, Validation Accuracy: 76.6000\n",
            "Epoch 60/100, Loss: 0.1956, Validation Accuracy: 77.6000\n",
            "Epoch 80/100, Loss: 0.1135, Validation Accuracy: 75.8000\n",
            "Epoch 100/100, Loss: 0.0714, Validation Accuracy: 76.2000\n",
            "Run 1 Test Accuracy: 75.70%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 3, Run: 2\n",
            "Epoch 20/100, Loss: 0.8992, Validation Accuracy: 72.8000\n",
            "Epoch 40/100, Loss: 0.4312, Validation Accuracy: 77.6000\n",
            "Epoch 60/100, Loss: 0.2324, Validation Accuracy: 75.8000\n",
            "Epoch 80/100, Loss: 0.1521, Validation Accuracy: 76.2000\n",
            "Epoch 100/100, Loss: 0.0999, Validation Accuracy: 76.0000\n",
            "Run 2 Test Accuracy: 76.00%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 3, Run: 3\n",
            "Epoch 20/100, Loss: 0.8810, Validation Accuracy: 74.2000\n",
            "Epoch 40/100, Loss: 0.4068, Validation Accuracy: 73.6000\n",
            "Epoch 60/100, Loss: 0.2883, Validation Accuracy: 77.0000\n",
            "Epoch 80/100, Loss: 0.1095, Validation Accuracy: 74.4000\n",
            "Epoch 100/100, Loss: 0.0737, Validation Accuracy: 75.6000\n",
            "Run 3 Test Accuracy: 74.90%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 3, Run: 4\n",
            "Epoch 20/100, Loss: 0.8558, Validation Accuracy: 70.6000\n",
            "Epoch 40/100, Loss: 0.4218, Validation Accuracy: 74.6000\n",
            "Epoch 60/100, Loss: 0.2391, Validation Accuracy: 74.2000\n",
            "Epoch 80/100, Loss: 0.1821, Validation Accuracy: 72.8000\n",
            "Epoch 100/100, Loss: 0.0660, Validation Accuracy: 74.0000\n",
            "Run 4 Test Accuracy: 74.50%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 8, Run: 1\n",
            "Epoch 20/100, Loss: 1.9209, Validation Accuracy: 7.8000\n",
            "Epoch 40/100, Loss: 1.8931, Validation Accuracy: 11.4000\n",
            "Epoch 60/100, Loss: 1.7844, Validation Accuracy: 23.6000\n",
            "Epoch 80/100, Loss: 1.6985, Validation Accuracy: 22.4000\n",
            "Epoch 100/100, Loss: 1.4989, Validation Accuracy: 36.4000\n",
            "Run 1 Test Accuracy: 34.60%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 8, Run: 2\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 16.2000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 16.2000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Run 2 Test Accuracy: 6.40%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 8, Run: 3\n",
            "Epoch 20/100, Loss: 1.9192, Validation Accuracy: 7.8000\n",
            "Epoch 40/100, Loss: 1.8638, Validation Accuracy: 10.8000\n",
            "Epoch 60/100, Loss: 1.9101, Validation Accuracy: 18.8000\n",
            "Epoch 80/100, Loss: 1.4899, Validation Accuracy: 30.8000\n",
            "Epoch 100/100, Loss: 1.3666, Validation Accuracy: 32.4000\n",
            "Run 3 Test Accuracy: 31.40%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 8, Run: 4\n",
            "Epoch 20/100, Loss: 1.9459, Validation Accuracy: 15.6000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 15.6000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Run 4 Test Accuracy: 13.00%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 10, Run: 1\n",
            "Epoch 20/100, Loss: 1.9461, Validation Accuracy: 5.8000\n",
            "Epoch 40/100, Loss: 1.9460, Validation Accuracy: 15.6000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Run 1 Test Accuracy: 13.00%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 10, Run: 2\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 5.8000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 15.6000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 31.6000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Run 2 Test Accuracy: 6.40%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 10, Run: 3\n",
            "Epoch 20/100, Loss: 1.9461, Validation Accuracy: 16.2000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 31.6000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 11.4000\n",
            "Run 3 Test Accuracy: 10.30%\n",
            "\n",
            "Running Model: WSP_GCN_4hop_no_residual, Layers: 10, Run: 4\n",
            "Epoch 20/100, Loss: 1.9460, Validation Accuracy: 31.6000\n",
            "Epoch 40/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Epoch 60/100, Loss: 1.9459, Validation Accuracy: 5.8000\n",
            "Epoch 80/100, Loss: 1.9459, Validation Accuracy: 12.2000\n",
            "Epoch 100/100, Loss: 1.9459, Validation Accuracy: 7.2000\n",
            "Run 4 Test Accuracy: 9.10%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 1, Run: 1\n",
            "Epoch 20/100, Loss: 0.0719, Validation Accuracy: 72.6000\n",
            "Epoch 40/100, Loss: 0.0092, Validation Accuracy: 71.2000\n",
            "Epoch 60/100, Loss: 0.0085, Validation Accuracy: 70.6000\n",
            "Epoch 80/100, Loss: 0.0122, Validation Accuracy: 70.2000\n",
            "Epoch 100/100, Loss: 0.0121, Validation Accuracy: 70.0000\n",
            "Run 1 Test Accuracy: 72.10%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 1, Run: 2\n",
            "Epoch 20/100, Loss: 0.0736, Validation Accuracy: 72.6000\n",
            "Epoch 40/100, Loss: 0.0099, Validation Accuracy: 72.4000\n",
            "Epoch 60/100, Loss: 0.0098, Validation Accuracy: 71.6000\n",
            "Epoch 80/100, Loss: 0.0153, Validation Accuracy: 70.0000\n",
            "Epoch 100/100, Loss: 0.0110, Validation Accuracy: 70.6000\n",
            "Run 2 Test Accuracy: 73.20%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 1, Run: 3\n",
            "Epoch 20/100, Loss: 0.0649, Validation Accuracy: 73.4000\n",
            "Epoch 40/100, Loss: 0.0090, Validation Accuracy: 71.8000\n",
            "Epoch 60/100, Loss: 0.0101, Validation Accuracy: 71.8000\n",
            "Epoch 80/100, Loss: 0.0121, Validation Accuracy: 70.8000\n",
            "Epoch 100/100, Loss: 0.0110, Validation Accuracy: 70.6000\n",
            "Run 3 Test Accuracy: 71.60%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 1, Run: 4\n",
            "Epoch 20/100, Loss: 0.0828, Validation Accuracy: 74.4000\n",
            "Epoch 40/100, Loss: 0.0115, Validation Accuracy: 74.0000\n",
            "Epoch 60/100, Loss: 0.0111, Validation Accuracy: 73.4000\n",
            "Epoch 80/100, Loss: 0.0124, Validation Accuracy: 71.2000\n",
            "Epoch 100/100, Loss: 0.0185, Validation Accuracy: 70.0000\n",
            "Run 4 Test Accuracy: 73.10%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 3, Run: 1\n",
            "Epoch 20/100, Loss: 0.1027, Validation Accuracy: 80.2000\n",
            "Epoch 40/100, Loss: 0.0054, Validation Accuracy: 78.8000\n",
            "Epoch 60/100, Loss: 0.0066, Validation Accuracy: 79.0000\n",
            "Epoch 80/100, Loss: 0.0155, Validation Accuracy: 78.0000\n",
            "Epoch 100/100, Loss: 0.0079, Validation Accuracy: 79.8000\n",
            "Run 1 Test Accuracy: 81.40%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 3, Run: 2\n",
            "Epoch 20/100, Loss: 0.0820, Validation Accuracy: 77.0000\n",
            "Epoch 40/100, Loss: 0.0053, Validation Accuracy: 74.8000\n",
            "Epoch 60/100, Loss: 0.0145, Validation Accuracy: 79.0000\n",
            "Epoch 80/100, Loss: 0.0037, Validation Accuracy: 79.8000\n",
            "Epoch 100/100, Loss: 0.0111, Validation Accuracy: 79.8000\n",
            "Run 2 Test Accuracy: 80.50%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 3, Run: 3\n",
            "Epoch 20/100, Loss: 0.0552, Validation Accuracy: 77.2000\n",
            "Epoch 40/100, Loss: 0.0094, Validation Accuracy: 78.0000\n",
            "Epoch 60/100, Loss: 0.0039, Validation Accuracy: 78.2000\n",
            "Epoch 80/100, Loss: 0.0086, Validation Accuracy: 77.6000\n",
            "Epoch 100/100, Loss: 0.0081, Validation Accuracy: 78.2000\n",
            "Run 3 Test Accuracy: 79.60%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 3, Run: 4\n",
            "Epoch 20/100, Loss: 0.1068, Validation Accuracy: 78.4000\n",
            "Epoch 40/100, Loss: 0.0094, Validation Accuracy: 76.0000\n",
            "Epoch 60/100, Loss: 0.0032, Validation Accuracy: 79.0000\n",
            "Epoch 80/100, Loss: 0.0113, Validation Accuracy: 77.8000\n",
            "Epoch 100/100, Loss: 0.0057, Validation Accuracy: 77.6000\n",
            "Run 4 Test Accuracy: 81.40%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 8, Run: 1\n",
            "Epoch 20/100, Loss: 0.5842, Validation Accuracy: 64.2000\n",
            "Epoch 40/100, Loss: 0.1139, Validation Accuracy: 77.0000\n",
            "Epoch 60/100, Loss: 0.0484, Validation Accuracy: 76.8000\n",
            "Epoch 80/100, Loss: 0.0325, Validation Accuracy: 77.0000\n",
            "Epoch 100/100, Loss: 0.0070, Validation Accuracy: 77.0000\n",
            "Run 1 Test Accuracy: 78.60%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 8, Run: 2\n",
            "Epoch 20/100, Loss: 0.7727, Validation Accuracy: 60.0000\n",
            "Epoch 40/100, Loss: 0.1640, Validation Accuracy: 72.6000\n",
            "Epoch 60/100, Loss: 0.0221, Validation Accuracy: 72.2000\n",
            "Epoch 80/100, Loss: 0.0463, Validation Accuracy: 73.4000\n",
            "Epoch 100/100, Loss: 0.0365, Validation Accuracy: 74.2000\n",
            "Run 2 Test Accuracy: 74.20%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 8, Run: 3\n",
            "Epoch 20/100, Loss: 0.7639, Validation Accuracy: 61.8000\n",
            "Epoch 40/100, Loss: 0.2275, Validation Accuracy: 72.8000\n",
            "Epoch 60/100, Loss: 0.0581, Validation Accuracy: 74.4000\n",
            "Epoch 80/100, Loss: 0.0313, Validation Accuracy: 74.4000\n",
            "Epoch 100/100, Loss: 0.0149, Validation Accuracy: 76.0000\n",
            "Run 3 Test Accuracy: 76.60%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 8, Run: 4\n",
            "Epoch 20/100, Loss: 0.7321, Validation Accuracy: 65.2000\n",
            "Epoch 40/100, Loss: 0.1370, Validation Accuracy: 77.2000\n",
            "Epoch 60/100, Loss: 0.0489, Validation Accuracy: 75.6000\n",
            "Epoch 80/100, Loss: 0.0115, Validation Accuracy: 72.4000\n",
            "Epoch 100/100, Loss: 0.0166, Validation Accuracy: 72.4000\n",
            "Run 4 Test Accuracy: 74.70%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 10, Run: 1\n",
            "Epoch 20/100, Loss: 0.9229, Validation Accuracy: 47.0000\n",
            "Epoch 40/100, Loss: 0.5570, Validation Accuracy: 58.4000\n",
            "Epoch 60/100, Loss: 0.2370, Validation Accuracy: 70.0000\n",
            "Epoch 80/100, Loss: 0.1526, Validation Accuracy: 73.8000\n",
            "Epoch 100/100, Loss: 0.1327, Validation Accuracy: 75.8000\n",
            "Run 1 Test Accuracy: 77.10%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 10, Run: 2\n",
            "Epoch 20/100, Loss: 1.0442, Validation Accuracy: 42.2000\n",
            "Epoch 40/100, Loss: 0.3022, Validation Accuracy: 58.2000\n",
            "Epoch 60/100, Loss: 0.1520, Validation Accuracy: 74.8000\n",
            "Epoch 80/100, Loss: 0.0307, Validation Accuracy: 72.6000\n",
            "Epoch 100/100, Loss: 0.0914, Validation Accuracy: 75.2000\n",
            "Run 2 Test Accuracy: 76.90%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 10, Run: 3\n",
            "Epoch 20/100, Loss: 1.2563, Validation Accuracy: 36.2000\n",
            "Epoch 40/100, Loss: 0.2822, Validation Accuracy: 65.8000\n",
            "Epoch 60/100, Loss: 0.1111, Validation Accuracy: 72.0000\n",
            "Epoch 80/100, Loss: 0.0423, Validation Accuracy: 70.0000\n",
            "Epoch 100/100, Loss: 0.0344, Validation Accuracy: 74.2000\n",
            "Run 3 Test Accuracy: 73.80%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_fixed, Layers: 10, Run: 4\n",
            "Epoch 20/100, Loss: 0.8195, Validation Accuracy: 60.2000\n",
            "Epoch 40/100, Loss: 0.3971, Validation Accuracy: 74.8000\n",
            "Epoch 60/100, Loss: 0.1612, Validation Accuracy: 75.4000\n",
            "Epoch 80/100, Loss: 0.0320, Validation Accuracy: 75.4000\n",
            "Epoch 100/100, Loss: 0.0112, Validation Accuracy: 74.0000\n",
            "Run 4 Test Accuracy: 72.80%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_learnable, Layers: 1, Run: 1\n",
            "Epoch 20/100, Loss: 0.2040, Validation Accuracy: 76.8000\n",
            "Epoch 40/100, Loss: 0.0138, Validation Accuracy: 77.0000\n",
            "Epoch 60/100, Loss: 0.0166, Validation Accuracy: 76.8000\n",
            "Epoch 80/100, Loss: 0.0236, Validation Accuracy: 76.8000\n",
            "Epoch 100/100, Loss: 0.0170, Validation Accuracy: 77.4000\n",
            "Run 1 Test Accuracy: 79.10%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_learnable, Layers: 1, Run: 2\n",
            "Epoch 20/100, Loss: 0.2000, Validation Accuracy: 78.6000\n",
            "Epoch 40/100, Loss: 0.0207, Validation Accuracy: 77.0000\n",
            "Epoch 60/100, Loss: 0.0182, Validation Accuracy: 78.0000\n",
            "Epoch 80/100, Loss: 0.0158, Validation Accuracy: 77.2000\n",
            "Epoch 100/100, Loss: 0.0185, Validation Accuracy: 77.2000\n",
            "Run 2 Test Accuracy: 78.80%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_learnable, Layers: 1, Run: 3\n",
            "Epoch 20/100, Loss: 0.2179, Validation Accuracy: 76.4000\n",
            "Epoch 40/100, Loss: 0.0126, Validation Accuracy: 75.8000\n",
            "Epoch 60/100, Loss: 0.0196, Validation Accuracy: 75.8000\n",
            "Epoch 80/100, Loss: 0.0238, Validation Accuracy: 76.0000\n",
            "Epoch 100/100, Loss: 0.0190, Validation Accuracy: 76.2000\n",
            "Run 3 Test Accuracy: 79.10%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_learnable, Layers: 1, Run: 4\n",
            "Epoch 20/100, Loss: 0.2071, Validation Accuracy: 79.0000\n",
            "Epoch 40/100, Loss: 0.0144, Validation Accuracy: 76.4000\n",
            "Epoch 60/100, Loss: 0.0168, Validation Accuracy: 76.4000\n",
            "Epoch 80/100, Loss: 0.0224, Validation Accuracy: 76.6000\n",
            "Epoch 100/100, Loss: 0.0182, Validation Accuracy: 76.8000\n",
            "Run 4 Test Accuracy: 79.20%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_learnable, Layers: 3, Run: 1\n",
            "Epoch 20/100, Loss: 0.5393, Validation Accuracy: 73.0000\n",
            "Epoch 40/100, Loss: 0.0989, Validation Accuracy: 73.2000\n",
            "Epoch 60/100, Loss: 0.0120, Validation Accuracy: 73.4000\n",
            "Epoch 80/100, Loss: 0.0289, Validation Accuracy: 72.0000\n",
            "Epoch 100/100, Loss: 0.0106, Validation Accuracy: 72.8000\n",
            "Run 1 Test Accuracy: 75.10%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_learnable, Layers: 3, Run: 2\n",
            "Epoch 20/100, Loss: 0.6230, Validation Accuracy: 78.8000\n",
            "Epoch 40/100, Loss: 0.0544, Validation Accuracy: 75.0000\n",
            "Epoch 60/100, Loss: 0.0243, Validation Accuracy: 74.4000\n",
            "Epoch 80/100, Loss: 0.0140, Validation Accuracy: 74.6000\n",
            "Epoch 100/100, Loss: 0.0188, Validation Accuracy: 74.8000\n",
            "Run 2 Test Accuracy: 75.60%\n",
            "\n",
            "Running Model: WSP_GCN_2hop_residual_learnable, Layers: 3, Run: 3\n",
            "Epoch 20/100, Loss: 0.5734, Validation Accuracy: 60.0000\n",
            "Epoch 40/100, Loss: 0.0290, Validation Accuracy: 74.6000\n",
            "Epoch 60/100, Loss: 0.0121, Validation Accuracy: 75.8000\n"
          ]
        }
      ],
      "source": [
        "# Global results dictionary to store all results\n",
        "results = {}\n",
        "\n",
        "# Iterate over each model configuration\n",
        "for model_key, model_config in models.items():\n",
        "    model_name = model_key\n",
        "    num_hops = model_config[\"num_hops\"]\n",
        "\n",
        "    if model_key.startswith(\"WSP_GCN\"):\n",
        "      use_residual = model_config[\"use_residual\"]\n",
        "      learnable_residual_scale = model_config[\"learnable_residual_scale\"]\n",
        "\n",
        "    # Initialize a nested dictionary for this model\n",
        "    results[model_name] = {}\n",
        "\n",
        "    # Iterate over each layer count\n",
        "    for n_layers in layer_counts:\n",
        "        # Initialize a nested dictionary for this layer configuration\n",
        "        results[model_name][n_layers] = {}\n",
        "\n",
        "        # Define the parameters for this configuration\n",
        "        training_params = base_training_params.copy()\n",
        "        training_params.update({\n",
        "            \"model_name\": model_name,\n",
        "            \"n_layers\": n_layers,\n",
        "            \"use_residual\": use_residual,\n",
        "            \"learnable_residual_scale\": learnable_residual_scale\n",
        "        })\n",
        "\n",
        "        # Get the precomputed normalized adjacency list for the current number of hops\n",
        "        training_params[\"num_hops\"] = num_hops\n",
        "\n",
        "        # Run multiple trials for statistical significance\n",
        "        for run in range(1, num_runs + 1):\n",
        "            print(f\"\\nRunning Model: {model_key}, Layers: {n_layers}, Run: {run}\")\n",
        "\n",
        "            # Initialize storage for this run\n",
        "            results[model_name][n_layers][run] = {\n",
        "                \"val_accuracies\": [],\n",
        "                \"final_test_accuracy\": None,\n",
        "                \"val_node_embeddings\": None,\n",
        "                \"weights_over_epochs\": {}\n",
        "            }\n",
        "\n",
        "            # Add 'run' to training_params\n",
        "            training_params[\"run\"] = run\n",
        "\n",
        "            # Train the model\n",
        "            model = train(training_params, A_norm_d_list)\n",
        "\n",
        "            # Validation accuracies and weights are collected during training\n",
        "            if model_key.startswith(\"WSP_GCN\"):\n",
        "                test_accuracy = evaluate(model, dataset.data, dataset.data.test_mask, A_norm_d_list)\n",
        "            else:\n",
        "                test_accuracy = evaluate(model, dataset.data, dataset.data.test_mask)\n",
        "\n",
        "            # Store final test accuracy\n",
        "            results[model_name][n_layers][run][\"final_test_accuracy\"] = test_accuracy\n",
        "\n",
        "            # Generate and store node embeddings\n",
        "            model.eval()\n",
        "            x = dataset.data.x.to(device)\n",
        "            if model_key.startswith(\"WSP_GCN\"):\n",
        "                val_embeddings = model.generate_node_embeddings(x, A_norm_d_list)[val_mask]\n",
        "            else:\n",
        "                val_embeddings = model.generate_node_embeddings(x, dataset.data.edge_index)[val_mask]\n",
        "\n",
        "            # Convert node_embeddings to a list for JSON serialization\n",
        "            results[model_name][n_layers][run][\"val_node_embeddings\"] = val_embeddings.detach().cpu().numpy().tolist()\n",
        "\n",
        "            print(f\"Run {run} Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Save the results\n",
        "with open(\"results.json\", \"w\") as f:\n",
        "    json.dump(results, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFfbWGccec0u"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the results from the JSON file\n",
        "with open(\"results.json\", \"r\") as f:\n",
        "    results = json.load(f)"
      ],
      "metadata": {
        "id": "wqprVcz6f-N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeAyDsGJnaU"
      },
      "source": [
        "### Visualization #1: Final Test Accuracies with Standard Deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exz6vv0SKB86"
      },
      "outputs": [],
      "source": [
        "# Initialize a list to store the summary data\n",
        "summary_data = []\n",
        "\n",
        "# Iterate over each model in the results\n",
        "for model_name, layers_dict in results.items():\n",
        "    for n_layers, runs_dict in layers_dict.items():\n",
        "        # Initialize a list to store test accuracies for all runs\n",
        "        test_accuracies = []\n",
        "        num_hops = None\n",
        "\n",
        "        for run_num, run_data in runs_dict.items():\n",
        "            test_acc = run_data.get(\"final_test_accuracy\", None)\n",
        "            if test_acc is not None:\n",
        "                test_accuracies.append(test_acc)\n",
        "\n",
        "            # Extract number of hops for WSP_GCN\n",
        "            if model_name.startswith(\"WSP_GCN\") and \"num_hops\" in run_data:\n",
        "                num_hops = run_data.get(\"num_hops\", None)\n",
        "\n",
        "        # Compute mean and standard deviation if there are multiple runs\n",
        "        if len(test_accuracies) > 0:\n",
        "            mean_acc = np.mean(test_accuracies)\n",
        "            std_acc = np.std(test_accuracies)\n",
        "        else:\n",
        "            mean_acc = None\n",
        "            std_acc = None\n",
        "\n",
        "        # Append the data to the summary list\n",
        "        summary_data.append({\n",
        "            \"Model\": model_name,\n",
        "            \"Number_of_Layers\": n_layers,\n",
        "            \"Num_Hops\": num_hops if model_name == \"WSP_GCN\" else \"N/A\",\n",
        "            \"Mean_Test_Accuracy (%)\": round(mean_acc, 2) if mean_acc is not None else \"N/A\",\n",
        "            \"Std_Dev_Test_Accuracy (%)\": round(std_acc, 2) if std_acc is not None else \"N/A\"\n",
        "        })\n",
        "\n",
        "# Create a DataFrame from the summary data\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Sort the DataFrame for better readability\n",
        "summary_df = summary_df.sort_values(by=[\"Model\", \"Number_of_Layers\"])\n",
        "\n",
        "# Print line-by-line results\n",
        "print(\"\\nLine-by-line summary of final accuracies and standard deviations:\")\n",
        "for index, row in summary_df.iterrows():\n",
        "    print(\n",
        "        f\"Model: {row['Model']}, \"\n",
        "        f\"Number of Layers: {row['Number_of_Layers']}, \"\n",
        "        f\"Num Hops: {row['Num_Hops']}, \"\n",
        "        f\"Mean Test Accuracy: {row['Mean_Test_Accuracy (%)']}%, \"\n",
        "        f\"Std Dev: {row['Std_Dev_Test_Accuracy (%)']}%\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization #2: Validation over Epochs"
      ],
      "metadata": {
        "id": "00RPVQBnZWAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Models and configurations\n",
        "models_to_plot = [\"GCN\", \"WSP_GCN_2hop_residual_learnable\", \"SkipGCN\", \"JumpKnowGCN\"]\n",
        "layers_to_plot = [1, 3, 8, 10]  # Layers to include in the visualization\n",
        "\n",
        "# Setup LaTeX\n",
        "plt.rc('text', usetex=True)\n",
        "plt.rc('font', family='serif')\n",
        "\n",
        "# Prepare subplots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
        "axs = axs.flatten()\n",
        "\n",
        "# To store handles and labels for the legend\n",
        "handles = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over models\n",
        "for idx, model_name in enumerate(models_to_plot):\n",
        "    ax = axs[idx]\n",
        "    for n_layers in layers_to_plot:\n",
        "        # Collect validation accuracy data for the specific model and layer\n",
        "        val_accuracies = []\n",
        "        for run_data in results.get(model_name, {}).get(str(n_layers), {}).values():\n",
        "            val_acc = run_data.get(\"val_accuracies\", [])\n",
        "            if val_acc:\n",
        "                val_accuracies.append(val_acc)\n",
        "\n",
        "        if val_accuracies:\n",
        "            val_accuracies = np.array(val_accuracies)\n",
        "            mean_acc = val_accuracies.mean(axis=0)\n",
        "            min_acc = val_accuracies.min(axis=0)\n",
        "            max_acc = val_accuracies.max(axis=0)\n",
        "\n",
        "            # Plot mean line and shaded area\n",
        "            epochs = np.arange(1, len(mean_acc) + 1)\n",
        "            line, = ax.plot(epochs, mean_acc, label=f\"{n_layers} Layers\")\n",
        "            ax.fill_between(epochs, min_acc, max_acc, alpha=0.2)\n",
        "\n",
        "            # Collect handles and labels only once for the legend\n",
        "            if idx == 0:\n",
        "                handles.append(line)\n",
        "                labels.append(f\"{n_layers} Layers\")\n",
        "\n",
        "    ax.set_title(model_name.replace(\"_\", \" \"), fontsize=12)\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "# Add a single shared legend\n",
        "fig.legend(handles, labels, loc='upper center', ncol=len(layers_to_plot), fontsize=10)\n",
        "\n",
        "# Add shared x and y labels\n",
        "fig.text(0.5, 0.04, \"Epochs\", ha='center', fontsize=12)\n",
        "fig.text(0.04, 0.5, \"Validation Accuracy\", va='center', rotation='vertical', fontsize=12)\n",
        "\n",
        "# Adjust layout to make space for the shared labels and legend\n",
        "plt.tight_layout(rect=[0.05, 0.05, 1, 0.95])\n",
        "\n",
        "plt.savefig(\"validation_accuracy_plots.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SiWvp-hfZfjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization #3: UMAP Visualizations"
      ],
      "metadata": {
        "id": "tiFWPERDgPDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "\n",
        "# Define the model and layers to plot\n",
        "model_name = \"WSP_GCN_2hop_residual_learnable\"\n",
        "layers_to_plot = [1, 3, 8, 10]\n",
        "\n",
        "# Initialize a dictionary to store the DataFrames for each layer\n",
        "feature_dict = {}\n",
        "\n",
        "# Iterate over the specified layers\n",
        "for n_layers in layers_to_plot:\n",
        "    # Access the runs for this model and layer\n",
        "    runs_dict = results.get(model_name, {}).get(str(n_layers), {})\n",
        "    if not runs_dict:\n",
        "        print(f\"No data found for {model_name} with {n_layers} layers.\")\n",
        "        continue\n",
        "\n",
        "    # Select the first run for visualization\n",
        "    first_run_key = next(iter(runs_dict.keys()), None)\n",
        "    if first_run_key is None:\n",
        "        print(f\"No runs found for {model_name} with {n_layers} layers.\")\n",
        "        continue\n",
        "\n",
        "    first_run = runs_dict[first_run_key]\n",
        "\n",
        "    val_embeddings = np.array(first_run.get(\"val_node_embeddings\", []))\n",
        "\n",
        "    if val_embeddings.size == 0:\n",
        "        print(f\"No embeddings found for {model_name} with {n_layers} layers in run {first_run_key}.\")\n",
        "        continue\n",
        "\n",
        "    # Access labels directly from the dataset\n",
        "    val_labels = dataset.data.y[val_mask].cpu().numpy()\n",
        "\n",
        "    # Check if the number of embeddings matches the number of labels\n",
        "    if len(val_embeddings) != len(val_labels):\n",
        "        print(f\"Mismatch between embeddings and labels for {model_name} with {n_layers} layers in run {first_run_key}.\")\n",
        "        continue\n",
        "\n",
        "    # Perform UMAP dimensionality reduction\n",
        "    try:\n",
        "        reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "        reduced_embeddings = reducer.fit_transform(val_embeddings)\n",
        "    except Exception as e:\n",
        "        print(f\"t-SNE failed for {model_name} with {n_layers} layers in run {first_run_key}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Create a DataFrame for visualization\n",
        "    df = pd.DataFrame(reduced_embeddings, columns=[\"Dimension 1\", \"Dimension 2\"])\n",
        "    df[\"labels\"] = val_labels\n",
        "\n",
        "    # Add to the feature dictionary\n",
        "    feature_dict[f\"{n_layers}_layer\"] = df"
      ],
      "metadata": {
        "id": "I2dCHb6U-jcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the visualization function\n",
        "def visualise(feature_dict: dict) -> None:\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Collect handles and labels for the shared legend\n",
        "    handles = []\n",
        "    labels = []\n",
        "\n",
        "    for idx, (key, df) in enumerate(feature_dict.items(), 1):\n",
        "        ax = plt.subplot(2, 2, idx)  # Create a 2x2 grid for visualization\n",
        "        unique_labels = sorted(df[\"labels\"].unique())  # Sort the labels in ascending order\n",
        "\n",
        "        # Plot each class and collect handles/labels\n",
        "        for label in unique_labels:\n",
        "            label_data = df[df[\"labels\"] == label]\n",
        "            scatter = ax.scatter(\n",
        "                label_data[\"Dimension 1\"],\n",
        "                label_data[\"Dimension 2\"],\n",
        "                label=f\"Label {label}\",\n",
        "                alpha=0.8,\n",
        "                s=20  # Adjust marker size as needed\n",
        "            )\n",
        "            if idx == 1:  # Collect handles and labels only once\n",
        "                handles.append(scatter)\n",
        "                labels.append(f\"Label {label}\")\n",
        "\n",
        "        ax.set_title(f\"{key.replace('_', ' ').capitalize()}\", fontsize=12)\n",
        "        ax.set_xlabel(\"Dimension 1\")\n",
        "        ax.set_ylabel(\"Dimension 2\")\n",
        "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "\n",
        "    # Create a single shared legend\n",
        "    plt.legend(\n",
        "        handles,\n",
        "        labels,\n",
        "        title=\"Labels\",\n",
        "        loc=\"center\",\n",
        "        bbox_to_anchor=(0.5, -0.175),  # Center below the plots\n",
        "        ncol=len(labels),\n",
        "        fontsize=10\n",
        "    )\n",
        "\n",
        "    # Adjust layout to make space for the legend\n",
        "    plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
        "\n",
        "    plt.savefig(\"umap_visualizations_2hop.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call the visualization function\n",
        "visualise(feature_dict)"
      ],
      "metadata": {
        "id": "n9NQgde0hvj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot visualizations for 1, 2 and 4-hop distances"
      ],
      "metadata": {
        "id": "VpSCceZ505HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your results.json file\n",
        "results_path = \"results.json\"\n",
        "\n",
        "# Define the model name and hops\n",
        "model_name = \"WSP_GCN\"\n",
        "hops = [1, 2, 4]\n",
        "layers_to_plot = [1, 3, 8, 10]\n",
        "\n",
        "# Define colors for labels\n",
        "unique_labels = dataset.data.y[val_mask].unique().cpu().numpy()\n",
        "num_classes = len(unique_labels)\n",
        "cmap = plt.get_cmap('tab10')\n",
        "label_colors = {label: cmap(i % 10) for i, label in enumerate(unique_labels)}\n",
        "\n",
        "# Define figure layout\n",
        "fig = plt.figure(figsize=(24, 18))\n",
        "from matplotlib import gridspec  # Import gridspec explicitly\n",
        "outer_grid = gridspec.GridSpec(len(hops), 1, wspace=0.2, hspace=0.3)\n",
        "\n",
        "\n",
        "\n",
        "for row, hop in enumerate(hops):\n",
        "    # Create a GridSpec for each hop\n",
        "    inner_grid = gridspec.GridSpecFromSubplotSpec(1, len(layers_to_plot), subplot_spec=outer_grid[row], wspace=0.3, hspace=0.3)\n",
        "\n",
        "    for col, layer in enumerate(layers_to_plot):\n",
        "        ax = plt.Subplot(fig, inner_grid[col])\n",
        "        fig.add_subplot(ax)\n",
        "\n",
        "        # Construct the key for the current hop\n",
        "        hop_key = f\"{hop}hop\" if hop !=1 else \"1hop\"\n",
        "        model_key = f\"{model_name}_{hop_key}\"\n",
        "\n",
        "        # Access the runs for this model, layer\n",
        "        runs_dict = results.get(model_key, {}).get(layer, {})\n",
        "        if not runs_dict:\n",
        "            print(f\"No data found for {model_key} with {layer} layers.\")\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f\"{hop}-hop, Layer {layer}\\nNo Data\", fontsize=12)\n",
        "            continue\n",
        "\n",
        "        # Select the first run for visualization\n",
        "        first_run_key = next(iter(runs_dict.keys()), None)\n",
        "        if first_run_key is None:\n",
        "            print(f\"No runs found for {model_key} with {layer} layers.\")\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f\"{hop}-hop, Layer {layer}\\nNo Runs\", fontsize=12)\n",
        "            continue\n",
        "\n",
        "        first_run = runs_dict[first_run_key]\n",
        "        val_embeddings = np.array(first_run.get(\"val_node_embeddings\", []))\n",
        "\n",
        "        if val_embeddings.size == 0:\n",
        "            print(f\"No embeddings found for {model_key} with {layer} layers in run {first_run_key}.\")\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f\"{hop}-hop, Layer {layer}\\nNo Embeddings\", fontsize=12)\n",
        "            continue\n",
        "\n",
        "        # Access labels directly from the dataset\n",
        "        val_labels = dataset.data.y[val_mask].cpu().numpy()\n",
        "\n",
        "        # Check if the number of embeddings matches the number of labels\n",
        "        if len(val_embeddings) != len(val_labels):\n",
        "            print(f\"Mismatch between embeddings and labels for {model_key} with {layer} layers in run {first_run_key}.\")\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f\"{hop}-hop, Layer {layer}\\nMismatch\", fontsize=12)\n",
        "            continue\n",
        "\n",
        "        # Perform UMAP dimensionality reduction\n",
        "        try:\n",
        "            reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "            reduced_embeddings = reducer.fit_transform(val_embeddings)\n",
        "        except Exception as e:\n",
        "            print(f\"UMAP failed for {model_key} with {layer} layers in run {first_run_key}: {e}\")\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f\"{hop}-hop, Layer {layer}\\nUMAP Failed\", fontsize=12)\n",
        "            continue\n",
        "\n",
        "        # Create a DataFrame for visualization\n",
        "        df = pd.DataFrame(reduced_embeddings, columns=[\"Dimension 1\", \"Dimension 2\"])\n",
        "        df[\"labels\"] = val_labels\n",
        "\n",
        "        # Plot each class separately\n",
        "        for label in unique_labels:\n",
        "            label_data = df[df[\"labels\"] == label]\n",
        "            ax.scatter(\n",
        "                label_data[\"Dimension 1\"],\n",
        "                label_data[\"Dimension 2\"],\n",
        "                label=f\"Class {label}\",\n",
        "                alpha=0.7,\n",
        "                s=30,\n",
        "                color=label_colors[label],\n",
        "                edgecolors='w',\n",
        "                linewidth=0.5\n",
        "            )\n",
        "\n",
        "        ax.set_title(f\"Layer {layer}\", fontsize=12)\n",
        "        ax.set_xlabel(\"Dimension 1\", fontsize=10)\n",
        "        ax.set_ylabel(\"Dimension 2\", fontsize=10)\n",
        "        ax.legend(loc='best', fontsize=8, markerscale=1.5)\n",
        "        ax.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "# Add a super title\n",
        "fig.suptitle(f\"t-SNE/UMAP Visualizations for {model_name} with Different Hops and Layers\", fontsize=20, y=0.95)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C0Nco3FiyANI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization #4: Silhouette Score and Davies-Bouldin Index for different models and layer depths"
      ],
      "metadata": {
        "id": "7uRtBHMy2vyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store the summary data\n",
        "summary_data = []\n",
        "\n",
        "# Iterate over each model in the results\n",
        "for model_key, layers_dict in results.items():\n",
        "    for n_layers, runs_dict in layers_dict.items():\n",
        "        # Select the first run for each model-layer configuration\n",
        "        first_run_key = next(iter(runs_dict.keys()), None)\n",
        "        if first_run_key is None:\n",
        "            print(f\"No runs found for {model_key} with {n_layers} layers.\")\n",
        "            continue\n",
        "\n",
        "        run_data = runs_dict[first_run_key]\n",
        "\n",
        "        # Extract node embeddings\n",
        "        val_embeddings = np.array(run_data.get(\"val_node_embeddings\", []))\n",
        "        if val_embeddings.size == 0:\n",
        "            print(f\"No embeddings found for {model_key} with {n_layers} layers in run {first_run_key}.\")\n",
        "            continue\n",
        "\n",
        "        # Access labels directly from the dataset\n",
        "        try:\n",
        "            val_labels = dataset.data.y[val_mask].cpu().numpy()\n",
        "        except NameError:\n",
        "            raise NameError(\"Ensure that 'dataset' and 'val_mask' are defined in your environment.\")\n",
        "\n",
        "        # Check if the number of embeddings matches the number of labels\n",
        "        if len(val_embeddings) != len(val_labels):\n",
        "            print(f\"Mismatch between embeddings and labels for {model_key} with {n_layers} layers in run {first_run_key}.\")\n",
        "            continue\n",
        "\n",
        "        # Compute Silhouette Score and Davies-Bouldin Index\n",
        "        try:\n",
        "            silhouette = silhouette_score(val_embeddings, val_labels)\n",
        "            db_index = davies_bouldin_score(val_embeddings, val_labels)\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing metrics for {model_key} with {n_layers} layers in run {first_run_key}: {e}\")\n",
        "            silhouette = None\n",
        "            db_index = None\n",
        "\n",
        "        # Extract Num_Hops from the model_key if applicable\n",
        "        if \"hop\" in model_key:\n",
        "            try:\n",
        "                num_hops_str = ''.join(filter(str.isdigit, model_key.split('_')[-1]))\n",
        "                num_hops = int(num_hops_str) if num_hops_str.isdigit() else \"N/A\"\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting number of hops from model_key '{model_key}': {e}\")\n",
        "                num_hops = \"N/A\"\n",
        "        else:\n",
        "            num_hops = \"N/A\"\n",
        "\n",
        "        # Append the metrics to the summary_data list\n",
        "        summary_data.append({\n",
        "            \"Model\": model_key,\n",
        "            \"Number_of_Layers\": n_layers,\n",
        "            \"Num_Hops\": num_hops,\n",
        "            \"Silhouette_Score\": round(silhouette, 4) if silhouette is not None else \"N/A\",\n",
        "            \"Davies-Bouldin_Index\": round(db_index, 4) if db_index is not None else \"N/A\"\n",
        "        })\n",
        "\n",
        "# Create a DataFrame from the summary data\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Sort the DataFrame for better readability\n",
        "summary_df = summary_df.sort_values(by=[\"Model\", \"Number_of_Layers\"]).reset_index(drop=True)\n",
        "\n",
        "# Display the DataFrame\n",
        "# print(summary_df)\n",
        "\n",
        "print(\"\\nLine-by-line summary of Silhouette Score and Davies-Bouldin Index:\")\n",
        "for index, row in summary_df.iterrows():\n",
        "    print(\n",
        "        f\"Model: {row['Model']}, \"\n",
        "        f\"Number of Layers: {row['Number_of_Layers']}, \"\n",
        "        f\"Num Hops: {row['Num_Hops']}, \"\n",
        "        f\"Silhouette Score: {row['Silhouette_Score']}, \"\n",
        "        f\"Davies-Bouldin Index: {row['Davies-Bouldin_Index']}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "MNyjig9Y3fdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization #5: Distribution of Final Weights"
      ],
      "metadata": {
        "id": "IjrLYu8N6jHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model configurations for 4-hop with residual\n",
        "model_names = {\n",
        "    \"WSP_GCN_4hop_residual_learnable\": 4\n",
        "}\n",
        "\n",
        "# Define the number of layers and selected layers for 4-hop\n",
        "n_layers_target = 10\n",
        "selected_layers = [1, 3, 8, 10]\n",
        "\n",
        "# Initialize dictionaries to store raw weights and residual scales for 4-hop\n",
        "weights_4hop_raw = {layer: {hop: [] for hop in range(1,5)} for layer in selected_layers}\n",
        "weights_4hop_residual = {layer: [] for layer in selected_layers}\n",
        "\n",
        "# Iterate over each model (only 4-hop)\n",
        "for model_key, num_hops in model_names.items():\n",
        "    if model_key not in results:\n",
        "        print(f\"Model {model_key} not found in results.\")\n",
        "        continue\n",
        "    for n_layers, runs_dict in results[model_key].items():\n",
        "        if int(n_layers) != n_layers_target:\n",
        "            continue  # Only consider the target number of layers (10)\n",
        "        for run_num, run_data in runs_dict.items():\n",
        "            weights_over_epochs = run_data.get(\"weights_over_epochs\", {})\n",
        "            if not weights_over_epochs:\n",
        "                print(f\"No weights_over_epochs found for {model_key}, layer {n_layers}, run {run_num}.\")\n",
        "                continue\n",
        "            # Get the last epoch\n",
        "            last_epoch = max(int(epoch) for epoch in weights_over_epochs.keys())\n",
        "            final_weights = weights_over_epochs[str(last_epoch)]  # dict: layer_idx: {\"raw_weights\": [...], \"residual_scale\": ...}\n",
        "            print(f\"Run {run_num}, Final Epoch {last_epoch}, Weights: {final_weights}\")\n",
        "            # Iterate over layers\n",
        "            for layer_idx_str, layer_weights in final_weights.items():\n",
        "                layer_idx = int(layer_idx_str) + 1  # Convert string index to integer and make it 1-based\n",
        "                if layer_idx not in selected_layers:\n",
        "                    continue  # Only consider selected layers\n",
        "                print(f\"Processing Layer: {layer_idx}\")\n",
        "                if model_key.startswith(\"WSP_GCN\"):\n",
        "                    # Collect raw_weights for hops 1 to 4\n",
        "                    for hop_idx in range(1, num_hops +1):\n",
        "                        try:\n",
        "                            w_i = float(layer_weights[\"raw_weights\"][hop_idx -1])\n",
        "                            sigmoid_w = torch.sigmoid(torch.tensor(w_i)).item()\n",
        "                            weights_4hop_raw[layer_idx][hop_idx].append(sigmoid_w)\n",
        "                            print(f\"Layer {layer_idx}, Hop {hop_idx}, Sigmoid Weight: {sigmoid_w}\")\n",
        "                        except (IndexError, ValueError, TypeError) as e:\n",
        "                            print(f\"Error processing raw_weights for {model_key}, layer {layer_idx}, hop {hop_idx}, run {run_num}: {e}\")\n",
        "                            continue\n",
        "                    # Collect residual_scale\n",
        "                    residual_scale = layer_weights.get(\"residual_scale\", None)\n",
        "                    if residual_scale is not None:\n",
        "                        try:\n",
        "                            residual_scale_val = float(residual_scale)\n",
        "                            weights_4hop_residual[layer_idx].append(residual_scale_val)\n",
        "                            print(f\"Layer {layer_idx}, Residual Scale: {residual_scale_val}\")\n",
        "                        except (ValueError, TypeError) as e:\n",
        "                            print(f\"Error processing residual_scale for {model_key}, layer {layer_idx}, run {run_num}: {e}\")\n",
        "                            continue"
      ],
      "metadata": {
        "id": "JxFdwX7z6wu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute mean and std for 4-hop raw_weights\n",
        "mean_4hop_raw = {layer: {hop: np.mean(weights_4hop_raw[layer][hop]) if weights_4hop_raw[layer][hop] else 0\n",
        "                         for hop in range(1,5)} for layer in selected_layers}\n",
        "std_4hop_raw = {layer: {hop: np.std(weights_4hop_raw[layer][hop]) if weights_4hop_raw[layer][hop] else 0\n",
        "                        for hop in range(1,5)} for layer in selected_layers}\n",
        "\n",
        "# Compute mean and std for residual_scale for 4-hop\n",
        "mean_4hop_residual = {layer: np.mean(weights_4hop_residual[layer]) if weights_4hop_residual[layer] else 0\n",
        "                      for layer in selected_layers}\n",
        "std_4hop_residual = {layer: np.std(weights_4hop_residual[layer]) if weights_4hop_residual[layer] else 0\n",
        "                     for layer in selected_layers}\n",
        "\n",
        "# Define colors for hops and residual_scale\n",
        "hop_colors = {\n",
        "    1: '#1f77b4',        # Blue\n",
        "    2: '#ff7f0e',        # Orange\n",
        "    3: '#2ca02c',        # Green\n",
        "    4: '#d62728',        # Red\n",
        "    'residual_scale': '#9467bd'  # Purple\n",
        "}\n",
        "\n",
        "# Create the figure and single subplot for 4-hop\n",
        "fig, ax = plt.subplots(figsize=(18, 6))\n",
        "\n",
        "# Plot raw_weights for hops 1 to 4\n",
        "bar_width = 0.15\n",
        "index = np.arange(len(selected_layers))\n",
        "offset = 0\n",
        "\n",
        "for hop in range(1,5):\n",
        "    means = [mean_4hop_raw[layer][hop] for layer in selected_layers]\n",
        "    stds = [std_4hop_raw[layer][hop] for layer in selected_layers]\n",
        "    ax.bar(index + (hop -1)*bar_width, means, bar_width, yerr=stds,\n",
        "           color=hop_colors[hop], alpha=0.7, capsize=5, label=f\"Hop {hop}\")\n",
        "\n",
        "# Plot residual_scale\n",
        "means_residual = [mean_4hop_residual[layer] for layer in selected_layers]\n",
        "stds_residual = [std_4hop_residual[layer] for layer in selected_layers]\n",
        "ax.bar(index + (4)*bar_width, means_residual, bar_width, yerr=stds_residual,\n",
        "       color=hop_colors['residual_scale'], alpha=0.7, capsize=5, label=\"Residual Scale\")\n",
        "\n",
        "# Set titles and labels\n",
        "ax.set_title(\"Distribution of Final Weights and Residual Scale in 4-hop WSP-GCN (10 Layers)\", fontsize=16)\n",
        "ax.set_xlabel(\"Layer\", fontsize=14)\n",
        "ax.set_ylabel(\"$w$\", fontsize=14)\n",
        "ax.set_xticks(index + (4)*bar_width / 2)\n",
        "ax.set_xticklabels(selected_layers)\n",
        "ax.legend(title=\"Components\", fontsize=12)\n",
        "ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"final_weights_distribution_4hop_with_residual_scale.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q1kTUAHbAm-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization #6: Weights over epochs"
      ],
      "metadata": {
        "id": "4rxveucwcAE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the specific model and configuration to visualize\n",
        "model_name = \"WSP_GCN_2hop_residual_learnable\"\n",
        "n_layers_target = \"10\"  # Layer count as string since JSON keys are strings\n",
        "n_hops = 2\n",
        "selected_layers = [1, 3, 6, 10]  # Layers to plot\n",
        "\n",
        "# Initialize a dictionary to store weights\n",
        "# Structure: { hop: { layer: { run: [w_i over epochs] } } }\n",
        "weights = {hop: {layer: {} for layer in selected_layers} for hop in range(1, n_hops+1)}\n",
        "residual_scales = {layer: {} for layer in selected_layers}  # Structure: { layer: { run: [residual_scale over epochs] } }\n",
        "\n",
        "# Access the runs for the specified model and layer count\n",
        "runs_dict = results.get(model_name, {}).get(str(n_layers_target), {})\n",
        "if not runs_dict:\n",
        "    print(f\"No runs found for {model_name} with {n_layers_target} layers.\")\n",
        "else:\n",
        "    for run_num, run_data in runs_dict.items():\n",
        "        weights_over_epochs = run_data.get(\"weights_over_epochs\", {})\n",
        "        if not weights_over_epochs:\n",
        "            print(f\"No weights_over_epochs found for {model_name}, layers {n_layers_target}, run {run_num}.\")\n",
        "            continue\n",
        "        # Sort epochs in ascending order\n",
        "        sorted_epochs = sorted(weights_over_epochs.keys(), key=lambda x: int(x))\n",
        "        for epoch in sorted_epochs:\n",
        "            layer_weights = weights_over_epochs[epoch]\n",
        "            for layer_idx_str, layer_weights_dict in layer_weights.items():\n",
        "                layer_idx = int(layer_idx_str) + 1  # Convert to 1-based index\n",
        "                if layer_idx not in selected_layers:\n",
        "                    continue\n",
        "                # Extract raw_weights and residual_scale\n",
        "                raw_weights = layer_weights_dict.get(\"raw_weights\", [])\n",
        "                residual_scale = layer_weights_dict.get(\"residual_scale\", None)\n",
        "\n",
        "                for hop in range(1, n_hops+1):\n",
        "                    try:\n",
        "                        w_i = float(raw_weights[hop-1])\n",
        "                        sigmoid_w = torch.sigmoid(torch.tensor(w_i)).item()\n",
        "                    except (IndexError, ValueError, TypeError):\n",
        "                        print(f\"Invalid raw_weight at Model: {model_name}, Layer: {layer_idx}, Hop: {hop}, Run: {run_num}, Epoch: {epoch}\")\n",
        "                        continue\n",
        "                    # Initialize run list if not present\n",
        "                    if run_num not in weights[hop][layer_idx]:\n",
        "                        weights[hop][layer_idx][run_num] = []\n",
        "                    weights[hop][layer_idx][run_num].append(sigmoid_w)\n",
        "\n",
        "                if residual_scale is not None:\n",
        "                    try:\n",
        "                        residual_scale_val = float(residual_scale)\n",
        "                    except (ValueError, TypeError):\n",
        "                        print(f\"Invalid residual_scale at Model: {model_name}, Layer: {layer_idx}, Run: {run_num}, Epoch: {epoch}\")\n",
        "                        continue\n",
        "                    # Initialize run list if not present\n",
        "                    if run_num not in residual_scales[layer_idx]:\n",
        "                        residual_scales[layer_idx][run_num] = []\n",
        "                    residual_scales[layer_idx][run_num].append(residual_scale_val)\n",
        "\n",
        "# Compute statistics: mean, min, max for each hop and layer over epochs\n",
        "# Structure for raw weights: { hop: { layer: { 'mean': [...], 'min': [...], 'max': [...] } } }\n",
        "weight_stats = {hop: {layer: {'mean': [], 'min': [], 'max': []} for layer in selected_layers} for hop in range(1, n_hops+1)}\n",
        "# Structure for residual scales: { layer: { 'mean': [...], 'min': [...], 'max': [...] } }\n",
        "residual_stats = {layer: {'mean': [], 'min': [], 'max': []} for layer in selected_layers}\n",
        "\n",
        "# Compute statistics for raw weights\n",
        "for hop in range(1, n_hops+1):\n",
        "    for layer in selected_layers:\n",
        "        runs = weights[hop][layer]  # { run_num: [w_i over epochs] }\n",
        "        if not runs:\n",
        "            print(f\"No raw_weights found for Hop: {hop}, Layer: {layer}\")\n",
        "            continue\n",
        "        # Convert runs to a NumPy array for easier computation\n",
        "        # Shape: [num_runs, num_epochs]\n",
        "        runs_array = np.array(list(runs.values()))\n",
        "        num_epochs = runs_array.shape[1]\n",
        "        for epoch_idx in range(num_epochs):\n",
        "            epoch_weights = runs_array[:, epoch_idx]\n",
        "            mean_w = np.mean(epoch_weights)\n",
        "            min_w = np.min(epoch_weights)\n",
        "            max_w = np.max(epoch_weights)\n",
        "            weight_stats[hop][layer]['mean'].append(mean_w)\n",
        "            weight_stats[hop][layer]['min'].append(min_w)\n",
        "            weight_stats[hop][layer]['max'].append(max_w)\n",
        "\n",
        "# Compute statistics for residual scales\n",
        "for layer in selected_layers:\n",
        "    runs = residual_scales[layer]  # { run_num: [residual_scale over epochs] }\n",
        "    if not runs:\n",
        "        print(f\"No residual_scales found for Layer: {layer}\")\n",
        "        continue\n",
        "    # Convert runs to a NumPy array for easier computation\n",
        "    # Shape: [num_runs, num_epochs]\n",
        "    runs_array = np.array(list(runs.values()))\n",
        "    num_epochs = runs_array.shape[1]\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        epoch_scales = runs_array[:, epoch_idx]\n",
        "        mean_scale = np.mean(epoch_scales)\n",
        "        min_scale = np.min(epoch_scales)\n",
        "        max_scale = np.max(epoch_scales)\n",
        "        residual_stats[layer]['mean'].append(mean_scale)\n",
        "        residual_stats[layer]['min'].append(min_scale)\n",
        "        residual_stats[layer]['max'].append(max_scale)\n",
        "\n",
        "print(\"Weight Statistics:\", weight_stats)\n",
        "print(\"Residual Scale Statistics:\", residual_stats)"
      ],
      "metadata": {
        "id": "uX8eIxroc-We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define distinct colors for each layer\n",
        "layer_colors = {\n",
        "    1: '#1f77b4',   # Blue\n",
        "    3: '#ff7f0e',   # Orange\n",
        "    6: '#2ca02c',   # Green\n",
        "    10: '#d62728',  # Red\n",
        "}\n",
        "\n",
        "# Create the figure with subplots\n",
        "# First row: Raw Weights for each hop\n",
        "# Second row: Residual Scale\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "# Define GridSpec with 2 rows: first for raw weights, second for residual scale\n",
        "gs = gridspec.GridSpec(2, n_hops, height_ratios=[3, 1], hspace=0.4)\n",
        "\n",
        "# Plot raw_weights for each hop\n",
        "for hop in range(1, n_hops+1):\n",
        "    ax = fig.add_subplot(gs[0, hop-1])\n",
        "    for layer in selected_layers:\n",
        "        stats = weight_stats[hop][layer]\n",
        "        if not stats['mean']:\n",
        "            continue\n",
        "        epochs = np.arange(1, len(stats['mean']) +1)\n",
        "        mean_w = stats['mean']\n",
        "        min_w = stats['min']\n",
        "        max_w = stats['max']\n",
        "        color = layer_colors.get(layer, '#000000')  # Default to black if layer not found\n",
        "        ax.plot(epochs, mean_w, label=f\"Layer {layer}\", color=color)\n",
        "        ax.fill_between(epochs, min_w, max_w, alpha=0.2, color=color)\n",
        "    ax.set_title(f\"Hop {hop}\", fontsize=14)\n",
        "    ax.set_xlabel(\"Epoch\", fontsize=12)\n",
        "    if hop ==1:\n",
        "        ax.set_ylabel(\"$w$\", fontsize=12)\n",
        "    ax.legend()\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Plot residual_scale in the second row, spanning all hops\n",
        "ax_residual = fig.add_subplot(gs[1, :])\n",
        "for layer in selected_layers:\n",
        "    stats = residual_stats[layer]\n",
        "    if not stats['mean']:\n",
        "        continue\n",
        "    epochs = np.arange(1, len(stats['mean']) +1)\n",
        "    mean_scale = stats['mean']\n",
        "    min_scale = stats['min']\n",
        "    max_scale = stats['max']\n",
        "    color = layer_colors.get(layer, '#000000')  # Default to black if layer not found\n",
        "    ax_residual.plot(epochs, mean_scale, label=f\"Layer {layer}\", color=color)\n",
        "    ax_residual.fill_between(epochs, min_scale, max_scale, alpha=0.2, color=color)\n",
        "\n",
        "ax_residual.set_title(\"Residual Scale\", fontsize=14)\n",
        "ax_residual.set_xlabel(\"Epoch\", fontsize=12)\n",
        "ax_residual.set_ylabel(\"$w$\", fontsize=12)\n",
        "ax_residual.legend()\n",
        "ax_residual.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Add a super title\n",
        "# plt.suptitle(f\"Evolution of Weights and Residual Scale over Epochs for {model_name} with {n_layers_target} Layers\", fontsize=16)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "plt.savefig(\"weights_and_residual_scale_evolution.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vnsze4yYexol"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QWKFAg_fAcF3",
        "QvsTj8ibBP3f",
        "jwXtZN9BCjB7",
        "hs3Rdco5TyO1",
        "ATOoFp0tThMa",
        "aS0Ga4V9BSGv",
        "qPHWdIMSCR7w",
        "00RPVQBnZWAF",
        "tiFWPERDgPDF",
        "VpSCceZ505HV",
        "IjrLYu8N6jHP"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}